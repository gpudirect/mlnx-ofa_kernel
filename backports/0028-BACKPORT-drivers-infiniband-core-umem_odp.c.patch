From: Talat Batheesh <talatb@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/umem_odp.c

Change-Id: I7521ac463c4930da1679b7cb2e1f3451e5602c1e
---
 drivers/infiniband/core/umem_odp.c | 146 +++++++++++++++++++++++++++++++++++--
 1 file changed, 140 insertions(+), 6 deletions(-)

--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#if HAVE_INTERVAL_TREE_GENERIC_H
 #include <linux/types.h>
 #include <linux/sched.h>
 #include <linux/sched/mm.h>
@@ -140,11 +141,46 @@ static void ib_umem_notifier_release(str
 	if (per_mm->active)
 		rbt_ib_umem_for_each_in_range(
 			&per_mm->umem_tree, 0, ULLONG_MAX,
+#if defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT)
 			ib_umem_notifier_release_trampoline, true, NULL);
+#else
+			ib_umem_notifier_release_trampoline, NULL);
+#endif
 	ib_invoke_sync_clients(mm, 0, 0);
 	up_read(&per_mm->umem_rwsem);
 }
 
+#ifdef HAVE_INVALIDATE_PAGE
+static int invalidate_page_trampoline(struct ib_umem_odp *item, u64 start,
+				      u64 end, void *cookie)
+{
+       ib_umem_notifier_start_account(item);
+       item->umem.context->invalidate_range(item, start, start + PAGE_SIZE);
+       ib_umem_notifier_end_account(item);
+       *(bool *)cookie = true;
+       return 0;
+}
+
+static void ib_umem_notifier_invalidate_page(struct mmu_notifier *mn,
+					      struct mm_struct *mm,
+					      unsigned long address)
+{
+	struct ib_ucontext_per_mm *per_mm =
+				container_of(mn, struct ib_ucontext_per_mm, mn);
+	bool call_rsync = false;
+
+	down_read(&per_mm->umem_rwsem);
+	if (per_mm->active)
+		rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, address,
+					      address + PAGE_SIZE,
+					      invalidate_page_trampoline, &call_rsync);
+	if (call_rsync)
+		ib_invoke_sync_clients(mm, address, PAGE_SIZE);
+
+	up_read(&per_mm->umem_rwsem);
+}
+#endif
+
 static int invalidate_range_start_trampoline(struct ib_umem_odp *item,
 					     u64 start, u64 end, void *cookie)
 {
@@ -154,18 +190,35 @@ static int invalidate_range_start_trampo
 	return 0;
 }
 
+#ifdef HAVE_MMU_NOTIFIER_RANGE_STRUCT
+static int ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
+					const struct mmu_notifier_range *range)
+#else
+#ifdef HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE
 static int ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
 						    struct mm_struct *mm,
 						    unsigned long start,
 						    unsigned long end,
 						    bool blockable)
+#else
+static void ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
+						    struct mm_struct *mm,
+						    unsigned long start,
+						    unsigned long end)
+#endif
+#endif /*HAVE_MMU_NOTIFIER_RANGE_STRUCT*/
 {
 	struct ib_ucontext_per_mm *per_mm =
 		container_of(mn, struct ib_ucontext_per_mm, mn);
 	bool call_rsync = false;
-	int ret;
 
+#if defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT)
+	int ret;
+#ifdef HAVE_MMU_NOTIFIER_RANGE_STRUCT
+	if (range->blockable)
+#else
 	if (blockable)
+#endif
 		down_read(&per_mm->umem_rwsem);
 	else if (!down_read_trylock(&per_mm->umem_rwsem))
 		return -EAGAIN;
@@ -179,14 +232,36 @@ static int ib_umem_notifier_invalidate_r
 		 */
 		return 0;
 	}
+#ifdef HAVE_MMU_NOTIFIER_RANGE_STRUCT
+	ret = rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, range->start,
+					    range->end,
+					    invalidate_range_start_trampoline,
+					    range->blockable, NULL);
+#else
 
 	ret = rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, start, end,
 					     invalidate_range_start_trampoline,
 					     blockable, &call_rsync);
+#endif
+
+#else /*defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT)*/ 
+
+	//ib_ucontext_notifier_start_account(context);
+	down_read(&per_mm->umem_rwsem);
+	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, start, end,
+			invalidate_range_start_trampoline,
+			&call_rsync);
+
+#endif
+#ifndef HAVE_MMU_NOTIFIER_RANGE_STRUCT
 	if (call_rsync)
 		ib_invoke_sync_clients(mm, start, end - start);
-
+#endif
+#if defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT)
 	return ret;
+#else
+	return;
+#endif
 }
 
 static int invalidate_range_end_trampoline(struct ib_umem_odp *item, u64 start,
@@ -197,19 +272,32 @@ static int invalidate_range_end_trampoli
 }
 
 static void ib_umem_notifier_invalidate_range_end(struct mmu_notifier *mn,
+#ifdef HAVE_MMU_NOTIFIER_RANGE_STRUCT
+						const struct mmu_notifier_range *range)
+#else
 						  struct mm_struct *mm,
 						  unsigned long start,
 						  unsigned long end)
+#endif
 {
 	struct ib_ucontext_per_mm *per_mm =
 		container_of(mn, struct ib_ucontext_per_mm, mn);
 
 	if (unlikely(!per_mm->active))
 		return;
-
-	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, start,
-				      end,
+	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree,
+#ifdef HAVE_MMU_NOTIFIER_RANGE_STRUCT
+					range->start,
+					range->end,
+#else 
+					 start,
+					 end,
+#endif/*HAVE_MMU_NOTIFIER_RANGE_STRUCT*/
+#if defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT)
 				      invalidate_range_end_trampoline, true, NULL);
+#else
+				      invalidate_range_end_trampoline, NULL);
+#endif/* defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT) */
 	up_read(&per_mm->umem_rwsem);
 }
 
@@ -239,6 +327,9 @@ static const struct mmu_notifier_ops ib_
 #ifdef CONFIG_CXL_LIB
 	.invalidate_range	    = ib_umem_notifier_invalidate_range,
 #endif
+#ifdef HAVE_INVALIDATE_PAGE
+	.invalidate_page            = ib_umem_notifier_invalidate_page,
+#endif
 };
 
 static void add_umem_to_per_mm(struct ib_umem_odp *umem_odp)
@@ -279,7 +370,11 @@ static struct ib_ucontext_per_mm *alloc_
 
 	per_mm->context = ctx;
 	per_mm->mm = mm;
-	per_mm->umem_tree = RB_ROOT_CACHED;
+#ifndef HAVE_INTERVAL_TREE_TAKES_RB_ROOT
+        per_mm->umem_tree = RB_ROOT_CACHED;
+#else
+	per_mm->umem_tree = RB_ROOT;
+#endif
 	init_rwsem(&per_mm->umem_rwsem);
 	per_mm->active = ctx->invalidate_range;
 
@@ -367,7 +462,9 @@ void put_per_mm(struct ib_umem_odp *umem
 	per_mm->active = false;
 	up_write(&per_mm->umem_rwsem);
 
+#ifndef HAVE_INTERVAL_TREE_TAKES_RB_ROOT
 	WARN_ON(!RB_EMPTY_ROOT(&per_mm->umem_tree.rb_root));
+#endif
 	mmu_notifier_unregister_no_release(&per_mm->mn, per_mm->mm);
 	put_pid(per_mm->tgid);
 	mmu_notifier_call_srcu(&per_mm->rcu, free_per_mm);
@@ -645,7 +742,9 @@ int ib_umem_odp_map_dma_pages(struct ib_
 	struct page       **local_page_list = NULL;
 	u64 page_mask, off;
 	int j, k, ret = 0, start_idx, npages = 0, page_shift;
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 	unsigned int flags = 0;
+#endif
 	phys_addr_t p = 0;
 
 	if (access_mask == 0)
@@ -676,8 +775,10 @@ int ib_umem_odp_map_dma_pages(struct ib_
 		goto out_put_task;
 	}
 
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 	if (access_mask & ODP_WRITE_ALLOWED_BIT)
 		flags |= FOLL_WRITE;
+#endif
 
 	start_idx = (user_virt - ib_umem_start(umem)) >> page_shift;
 	k = start_idx;
@@ -695,9 +796,29 @@ int ib_umem_odp_map_dma_pages(struct ib_
 		 * complex (and doesn't gain us much performance in most use
 		 * cases).
 		 */
+#if defined(HAVE_GET_USER_PAGES_REMOTE_8_PARAMS) || defined(HAVE_GET_USER_PAGES_REMOTE_7_PARAMS) || defined(HAVE_GET_USER_PAGES_REMOTE_8_PARAMS_W_LOCKED)
 		npages = get_user_pages_remote(owning_process, owning_mm,
 				user_virt, gup_num_pages,
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
+#ifdef HAVE_GET_USER_PAGES_REMOTE_8_PARAMS_W_LOCKED
 				flags, local_page_list, NULL, NULL);
+#else
+				flags, local_page_list, NULL);
+#endif
+#else
+				access_mask & ODP_WRITE_ALLOWED_BIT, 0,
+				local_page_list, NULL);
+#endif
+#else
+		npages = get_user_pages(owning_process, owning_mm,
+				user_virt, gup_num_pages,
+#ifdef HAVE_GET_USER_PAGES_7_PARAMS
+				flags, local_page_list, NULL);
+#else
+				access_mask & ODP_WRITE_ALLOWED_BIT,
+				0, local_page_list, NULL);
+#endif
+#endif
 		up_read(&owning_mm->mmap_sem);
 
 		if (npages < 0) {
@@ -828,10 +949,16 @@ EXPORT_SYMBOL(ib_umem_odp_unmap_dma_page
 /* @last is not a part of the interval. See comment for function
  * node_last.
  */
+#ifndef HAVE_INTERVAL_TREE_TAKES_RB_ROOT
 int rbt_ib_umem_for_each_in_range(struct rb_root_cached *root,
+#else
+int rbt_ib_umem_for_each_in_range(struct rb_root *root,
+#endif
 				  u64 start, u64 last,
 				  umem_call_back cb,
+#if defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT)
 				  bool blockable,
+#endif
 				  void *cookie)
 {
 	int ret_val = 0;
@@ -844,8 +971,10 @@ int rbt_ib_umem_for_each_in_range(struct
 	for (node = rbt_ib_umem_iter_first(root, start, last - 1);
 			node; node = next) {
 		/* TODO move the blockable decision up to the callback */
+#if defined(HAVE_UMEM_NOTIFIER_PARAM_BLOCKABLE) || defined(HAVE_MMU_NOTIFIER_RANGE_STRUCT)
 		if (!blockable)
 			return -EAGAIN;
+#endif
 		next = rbt_ib_umem_iter_next(node, start, last - 1);
 		umem = container_of(node, struct ib_umem_odp, interval_tree);
 		ret_val = cb(umem, start, last, cookie) || ret_val;
@@ -854,8 +983,13 @@ int rbt_ib_umem_for_each_in_range(struct
 	return ret_val;
 }
 EXPORT_SYMBOL(rbt_ib_umem_for_each_in_range);
+#endif
 
+#ifndef HAVE_INTERVAL_TREE_TAKES_RB_ROOT
 struct ib_umem_odp *rbt_ib_umem_lookup(struct rb_root_cached *root,
+#else
+struct ib_umem_odp *rbt_ib_umem_lookup(struct rb_root *root,
+#endif
 				       u64 addr, u64 length)
 {
 	struct umem_odp_node *node;
