From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/device.c

Change-Id: I804c30a3c3f66cfa50a0bb3ee10c1eeb3044890e
---
 drivers/infiniband/core/device.c | 127 ++++++++++++++++++++++++++++++++++++++-
 1 file changed, 124 insertions(+), 3 deletions(-)

--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -85,6 +85,7 @@ static LIST_HEAD(client_list);
 static DEFINE_MUTEX(device_mutex);
 static DECLARE_RWSEM(lists_rwsem);
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 static int ib_security_change(struct notifier_block *nb, unsigned long event,
 			      void *lsm_data);
 static void ib_policy_change_task(struct work_struct *work);
@@ -93,6 +94,7 @@ static DECLARE_WORK(ib_policy_change_wor
 static struct notifier_block ibdev_lsm_nb = {
 	.notifier_call = ib_security_change,
 };
+#endif
 
 static int ib_device_check_mandatory(struct ib_device *device)
 {
@@ -388,6 +390,7 @@ static int setup_port_pkey_list(struct i
 	return 0;
 }
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 static void ib_policy_change_task(struct work_struct *work)
 {
 	struct ib_device *dev;
@@ -422,6 +425,7 @@ static int ib_security_change(struct not
 
 	return NOTIFY_OK;
 }
+#endif /* HAVE_REGISTER_LSM_NOTIFIER */
 
 /**
  *	__dev_new_index	-	allocate an device index
@@ -463,6 +467,7 @@ int ib_register_device(struct ib_device
 	int ret;
 	struct ib_client *client;
 	struct ib_udata uhw = {.outlen = 0, .inlen = 0};
+#ifdef HAVE_DEVICE_DMA_OPS
 	struct device *parent = device->dev.parent;
 
 	WARN_ON_ONCE(device->dma_device);
@@ -494,6 +499,15 @@ int ib_register_device(struct ib_device
 		WARN_ON_ONCE(!parent);
 		device->dma_device = parent;
 	}
+#else /* HAVE_DEVICE_DMA_OPS */
+	WARN_ON_ONCE(!device->dev.parent && !device->dma_device);
+	WARN_ON_ONCE(device->dev.parent && device->dma_device
+		     && device->dev.parent != device->dma_device);
+	if (!device->dev.parent)
+		device->dev.parent = device->dma_device;
+	if (!device->dma_device)
+		device->dma_device = device->dev.parent;
+#endif /* HAVE_DEVICE_DMA_OPS */
 
 	mutex_lock(&device_mutex);
 
@@ -508,6 +522,10 @@ int ib_register_device(struct ib_device
 		goto out;
 	}
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	mutex_init(&device->skprio2up.lock);
+#endif
+
 	ret = read_port_immutable(device);
 	if (ret) {
 		pr_warn("Couldn't create per port immutable data %s\n",
@@ -527,11 +545,13 @@ int ib_register_device(struct ib_device
 		goto port_cleanup;
 	}
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_device_register_rdmacg(device);
 	if (ret) {
 		pr_warn("Couldn't register device with rdma cgroup\n");
 		goto cache_cleanup;
 	}
+#endif
 
 	memset(&device->attrs, 0, sizeof(device->attrs));
 	ret = device->query_device(device, &device->attrs, &uhw);
@@ -561,8 +581,10 @@ int ib_register_device(struct ib_device
 	return 0;
 
 cg_cleanup:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_device_unregister_rdmacg(device);
 cache_cleanup:
+#endif
 	ib_cache_cleanup_one(device);
 	ib_cache_release_one(device);
 port_cleanup:
@@ -601,7 +623,9 @@ void ib_unregister_device(struct ib_devi
 	}
 	up_read(&lists_rwsem);
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_device_unregister_rdmacg(device);
+#endif
 	ib_device_unregister_sysfs(device);
 
 	mutex_unlock(&device_mutex);
@@ -1055,6 +1079,46 @@ int ib_find_gid(struct ib_device *device
 }
 EXPORT_SYMBOL(ib_find_gid);
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+int ib_set_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    up >= NUM_UP ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	device->skprio2up.map[port_num - 1][prio] = up;
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_set_skprio2up);
+
+int ib_get_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 *up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    !up ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	*up = device->skprio2up.map[port_num - 1][prio];
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_get_skprio2up);
+#endif
+
 /**
  * ib_find_pkey - Returns the PKey table index where a specified
  *   PKey value occurs.
@@ -1145,11 +1209,19 @@ static const struct rdma_nl_cbs ibnl_ls_
 		.flags = RDMA_NL_ADMIN_PERM,
 	},
 	[RDMA_NL_LS_OP_SET_TIMEOUT] = {
+#ifdef HAVE_NETLINK_EXT_ACK
 		.doit = ib_nl_handle_set_timeout,
+#else
+		.dump = ib_nl_handle_set_timeout,
+#endif
 		.flags = RDMA_NL_ADMIN_PERM,
 	},
 	[RDMA_NL_LS_OP_IP_RESOLVE] = {
+#ifdef HAVE_NETLINK_EXT_ACK
 		.doit = ib_nl_handle_ip_res_resp,
+#else
+		.dump = ib_nl_handle_ip_res_resp,
+#endif
 		.flags = RDMA_NL_ADMIN_PERM,
 	},
 };
@@ -1162,17 +1234,60 @@ static int __init ib_core_init(void)
 	if (!ib_wq)
 		return -ENOMEM;
 
+#if defined(HAVE_ALLOC_WORKQUEUE)
 	ib_comp_wq = alloc_workqueue("ib-comp-wq",
-			WQ_HIGHPRI | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
+			0
+#if defined(HAVE_WQ_HIGHPRI)
+			| WQ_HIGHPRI
+#endif
+#if defined(HAVE_WQ_MEM_RECLAIM)
+			| WQ_MEM_RECLAIM
+#endif
+#if defined(HAVE_WQ_SYSFS)
+			| WQ_SYSFS
+#endif
+#if defined(HAVE_WQ_NON_REENTRANT)
+			| WQ_NON_REENTRANT
+#endif
+			, 0);
+#else /* HAVE_ALLOC_WORKQUEUE */
+	/* For older kernels that do not have WQ_NON_REENTRANT and
+	 * alloc_workqueue
+	 */
+	ib_comp_wq = create_singlethread_workqueue("ib-comp-wq");
+#endif /* HAVE_ALLOC_WORKQUEUE */
 	if (!ib_comp_wq) {
 		ret = -ENOMEM;
 		goto err;
 	}
 
+#if defined(HAVE_ALLOC_WORKQUEUE)
 	ib_comp_unbound_wq =
 		alloc_workqueue("ib-comp-unb-wq",
-				WQ_UNBOUND | WQ_HIGHPRI | WQ_MEM_RECLAIM |
-				WQ_SYSFS, WQ_UNBOUND_MAX_ACTIVE);
+			0
+#if defined(HAVE_WQ_UNBOUND_MAX_ACTIVE)
+			| WQ_UNBOUND | WQ_UNBOUND_MAX_ACTIVE
+#elif defined(HAVE_WQ_UNBOUND)
+			| WQ_UNBOUND
+#endif
+#if defined(HAVE_WQ_HIGHPRI)
+			| WQ_HIGHPRI
+#endif
+#if defined(HAVE_WQ_MEM_RECLAIM)
+			| WQ_MEM_RECLAIM
+#endif
+#if defined(HAVE_WQ_SYSFS)
+			| WQ_SYSFS
+#endif
+#if defined(HAVE_WQ_NON_REENTRANT)
+			| WQ_NON_REENTRANT
+#endif
+			, 0);
+#else /* HAVE_ALLOC_WORKQUEUE */
+	/* For older kernels that do not have alloc_workqueue
+	 */
+	ib_comp_unbound_wq = create_singlethread_workqueue("ib-comp-unb-wq");
+#endif /* HAVE_ALLOC_WORKQUEUE */
 	if (!ib_comp_unbound_wq) {
 		ret = -ENOMEM;
 		goto err_comp;
@@ -1208,11 +1323,13 @@ static int __init ib_core_init(void)
 		goto err_mad;
 	}
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 	ret = register_lsm_notifier(&ibdev_lsm_nb);
 	if (ret) {
 		pr_warn("Couldn't register LSM notifier. ret %d\n", ret);
 		goto err_sa;
 	}
+#endif
 
 	nldev_init();
 	rdma_nl_register(RDMA_NL_LS, ibnl_ls_cb_table);
@@ -1220,8 +1337,10 @@ static int __init ib_core_init(void)
 
 	return 0;
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 err_sa:
 	ib_sa_cleanup();
+#endif
 err_mad:
 	ib_mad_cleanup();
 err_addr:
@@ -1244,7 +1363,9 @@ static void __exit ib_core_cleanup(void)
 	roce_gid_mgmt_cleanup();
 	nldev_exit();
 	rdma_nl_unregister(RDMA_NL_LS);
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 	unregister_lsm_notifier(&ibdev_lsm_nb);
+#endif
 	ib_sa_cleanup();
 	ib_mad_cleanup();
 	addr_cleanup();
