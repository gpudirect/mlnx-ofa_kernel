From: Chris Mi <chrism@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_tc.c

Change-Id: I7a63a040b305daa0102762cafcf8c01c53b5634d
---
 drivers/net/ethernet/mellanox/mlx5/core/en_tc.c | 1040 ++++++++++++++++++++---
 1 file changed, 931 insertions(+), 109 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@ -11,14 +11,14 @@
  *     without modification, are permitted provided that the following
  *     conditions are met:
  *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
+ *	- Redistributions of source code must retain the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer.
  *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
+ *	- Redistributions in binary form must reproduce the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer in the documentation and/or other materials
+ *	  provided with the distribution.
  *
  * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
  * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
@@ -30,23 +30,47 @@
  * SOFTWARE.
  */
 
-#include <net/flow_dissector.h>
 #include <net/sch_generic.h>
 #include <net/pkt_cls.h>
+#ifdef HAVE_TC_GACT_H
 #include <net/tc_act/tc_gact.h>
+#endif
+#ifdef HAVE_IS_TCF_SKBEDIT_MARK
 #include <net/tc_act/tc_skbedit.h>
+#endif
 #include <linux/mlx5/fs.h>
 #include <linux/mlx5/device.h>
+#include <linux/mlx5/devcom.h>
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #include <linux/rhashtable.h>
 #include <linux/refcount.h>
+#endif
+#ifdef CONFIG_NET_SWITCHDEV
 #include <net/switchdev.h>
+#endif
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #include <net/tc_act/tc_mirred.h>
+#endif
+#ifdef HAVE_IS_TCF_VLAN
 #include <net/tc_act/tc_vlan.h>
+#endif
+#ifdef HAVE_TCF_TUNNEL_INFO
 #include <net/tc_act/tc_tunnel_key.h>
+#endif
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
+#include <linux/tc_act/tc_pedit.h>
 #include <net/tc_act/tc_pedit.h>
+#endif
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 #include <net/tc_act/tc_csum.h>
+#endif
+#ifdef HAVE_TCF_TUNNEL_INFO
 #include <net/vxlan.h>
+#endif
 #include <net/arp.h>
+#ifdef HAVE_TC_FLOWER_OFFLOAD
+#include <net/flow_dissector.h>
+#endif
 #include "en.h"
 #include "en_rep.h"
 #include "en_tc.h"
@@ -54,12 +78,77 @@
 #include "lib/vxlan.h"
 #include "fs_core.h"
 #include "en/port.h"
-#include <linux/mlx5/devcom.h>
+#include <linux/mlx5/vport.h>
+
+#if defined(HAVE_TC_FLOWER_OFFLOAD) && \
+    (!defined(HAVE_SWITCHDEV_PORT_SAME_PARENT_ID) || \
+    !defined(CONFIG_NET_SWITCHDEV))
+#include <net/bonding.h>
+static bool switchdev_port_same_parent_id(struct net_device *a,
+					  struct net_device *b)
+{
+	struct mlx5e_priv *priv_a, *priv_b;
+	struct mlx5_eswitch *peer_esw;
+	struct mlx5_devcom *devcom;
+	struct net_device *ndev;
+	struct bonding *bond;
+	bool ret = true;
+
+	if (netif_is_bond_master(b)) {
+		bond = netdev_priv(b);
+		if (!bond_has_slaves(bond))
+			return false;
+
+		rcu_read_lock();
+#ifdef for_each_netdev_in_bond_rcu
+		for_each_netdev_in_bond_rcu(b, ndev) {
+#else
+		for_each_netdev_in_bond(b, ndev) {
+#endif
+			ret &= switchdev_port_same_parent_id(a, ndev);
+			if (!ret)
+				break;
+		}
+		rcu_read_unlock();
+		return ret;
+	}
+
+	if (!(a->features & NETIF_F_HW_TC) || !(b->features & NETIF_F_HW_TC))
+		return false;
+
+	priv_a = netdev_priv(a);
+	priv_b = netdev_priv(b);
+
+	if (!priv_a->mdev->priv.eswitch || !priv_b->mdev->priv.eswitch)
+		return false;
+
+	if (priv_a->mdev->priv.eswitch->mode != SRIOV_OFFLOADS ||
+	    priv_b->mdev->priv.eswitch->mode != SRIOV_OFFLOADS)
+		return false;
+
+	if (priv_a->mdev == priv_b->mdev)
+		return true;
+
+	if (!mlx5_lag_is_sriov(priv_a->mdev))
+		return false;
+
+	devcom = priv_a->mdev->priv.devcom;
+	peer_esw = mlx5_devcom_get_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+	if (!peer_esw)
+		return false;
+
+	ret = (peer_esw->dev == priv_b->mdev);
+	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+	return ret;
+}
+#endif
 
 struct mlx5_nic_flow_attr {
 	u32 action;
 	u32 flow_tag;
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	u32 mod_hdr_id;
+#endif
 	u32 hairpin_tirn;
 	u8 match_level;
 	struct mlx5_flow_table	*hairpin_ft;
@@ -77,13 +166,14 @@ enum {
 	MLX5E_TC_FLOW_HAIRPIN	= BIT(MLX5E_TC_FLOW_BASE + 1),
 	MLX5E_TC_FLOW_HAIRPIN_RSS = BIT(MLX5E_TC_FLOW_BASE + 2),
 	MLX5E_TC_FLOW_SLOW	  = BIT(MLX5E_TC_FLOW_BASE + 3),
-	MLX5E_TC_FLOW_DUP         = BIT(MLX5E_TC_FLOW_BASE + 4),
+	MLX5E_TC_FLOW_DUP	  = BIT(MLX5E_TC_FLOW_BASE + 4),
 	MLX5E_TC_FLOW_NOT_READY   = BIT(MLX5E_TC_FLOW_BASE + 5),
 	MLX5E_TC_FLOW_INIT_DONE	  = BIT(MLX5E_TC_FLOW_BASE + 6),
 };
 
 #define MLX5E_TC_MAX_SPLITS 1
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 struct mlx5e_tc_flow {
 	struct rhash_head	node;
 	struct mlx5e_priv	*priv;
@@ -91,17 +181,25 @@ struct mlx5e_tc_flow {
 	atomic_t		flags;
 	struct mlx5_flow_handle *rule[MLX5E_TC_MAX_SPLITS + 1];
 	struct mlx5e_encap_entry *e; /* attached encap instance */
-	struct mlx5e_tc_flow    *peer_flow;
-	struct list_head	encap;   /* flows sharing the same encap ID */
+	struct mlx5e_tc_flow	*peer_flow;
+#ifdef HAVE_TCF_TUNNEL_INFO
+	struct list_head	encap;	 /* flows sharing the same encap ID */
+#endif
 	unsigned long encap_init_jiffies;
 	struct mlx5e_mod_hdr_entry *mh; /* attached mod header instance */
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	struct list_head	mod_hdr; /* flows sharing the same mod hdr ID */
+#endif
 	struct mlx5e_hairpin_entry *hpe; /* attached hairpin instance */
 	struct list_head	hairpin; /* flows sharing the same hairpin */
 	struct list_head	peer; /* flows with peer flow */
 	struct list_head	unready; /* flows not ready to be offloaded (e.g due to missing route) */
+#ifdef HAVE_REFCOUNT
 	refcount_t		refcnt;
-	struct list_head        tmp_list;
+#else
+	atomic_t		refcnt;
+#endif
+	struct list_head	tmp_list;
 	struct rcu_head		rcu_head;
 	union {
 		struct mlx5_esw_flow_attr esw_attr[0];
@@ -110,11 +208,15 @@ struct mlx5e_tc_flow {
 };
 
 struct mlx5e_tc_flow_parse_attr {
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct ip_tunnel_info tun_info;
+#endif
 	struct mlx5_flow_spec spec;
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	int num_mod_hdr_actions;
 	int max_mod_hdr_actions;
 	void *mod_hdr_actions;
+#endif
 	int mirred_ifindex;
 };
 
@@ -147,42 +249,30 @@ struct mlx5e_hairpin_entry {
 	u16 peer_vhca_id;
 	u8 prio;
 	struct mlx5e_hairpin *hp;
-	refcount_t refcnt;
-	struct rcu_head rcu;
-};
-
-struct mod_hdr_key {
-	int num_actions;
-	void *actions;
-};
-
-struct mlx5e_mod_hdr_entry {
-	/* a node of a hash table which keeps all the mod_hdr entries */
-	struct hlist_node mod_hdr_hlist;
-
-	/* protects flows list */
-	spinlock_t flows_lock;
-	/* flows sharing the same mod_hdr entry */
-	struct list_head flows;
-
-	struct mod_hdr_key key;
-
-	u32 mod_hdr_id;
-
-	refcount_t refcnt;
+#ifdef HAVE_REFCOUNT
+	refcount_t		refcnt;
+#else
+	atomic_t		refcnt;
+#endif
 	struct rcu_head rcu;
 };
 
-#define MLX5_MH_ACT_SZ MLX5_UN_SZ_BYTES(set_action_in_add_action_in_auto)
-
 static void mlx5e_tc_del_flow(struct mlx5e_priv *priv,
 			      struct mlx5e_tc_flow *flow);
 
 static struct mlx5e_tc_flow *mlx5e_flow_get(struct mlx5e_tc_flow *flow)
 {
 	if (!flow ||
+#ifdef HAVE_ATOMIC_READ_ACQUIRE
 	    !(atomic_read_acquire(&flow->flags) & MLX5E_TC_FLOW_INIT_DONE) ||
+#else
+	    !(smp_load_acquire(&flow->flags.counter) & MLX5E_TC_FLOW_INIT_DONE) ||
+#endif
+#ifdef HAVE_REFCOUNT
 	    !refcount_inc_not_zero(&flow->refcnt))
+#else
+	    !atomic_inc_not_zero(&flow->refcnt))
+#endif
 		return ERR_PTR(-EINVAL);
 	return flow;
 }
@@ -190,7 +280,11 @@ static struct mlx5e_tc_flow *mlx5e_flow_
 static void mlx5e_flow_put(struct mlx5e_priv *priv,
 			   struct mlx5e_tc_flow *flow)
 {
+#ifdef HAVE_REFCOUNT
 	if (refcount_dec_and_test(&flow->refcnt)) {
+#else
+	if (atomic_dec_and_test(&flow->refcnt)) {
+#endif
 		mlx5e_tc_del_flow(priv, flow);
 		kfree_rcu(flow, rcu_head);
 	}
@@ -203,7 +297,11 @@ static bool mlx5e_is_eswitch_flow(struct
 
 static bool mlx5e_is_offloaded_flow(struct mlx5e_tc_flow *flow)
 {
+#ifdef HAVE_ATOMIC_READ_ACQUIRE
 	return !!(atomic_read_acquire(&flow->flags) & MLX5E_TC_FLOW_OFFLOADED);
+#else
+	return !!(smp_load_acquire(&flow->flags.counter) & MLX5E_TC_FLOW_OFFLOADED);
+#endif
 }
 
 static void mlx5e_set_flow_flag_mb_before(struct mlx5e_tc_flow *flow, int flag)
@@ -213,6 +311,35 @@ static void mlx5e_set_flow_flag_mb_befor
 	atomic_or(flag, &flow->flags);
 }
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
+struct mod_hdr_key {
+	int num_actions;
+	void *actions;
+};
+
+struct mlx5e_mod_hdr_entry {
+	/* a node of a hash table which keeps all the mod_hdr entries */
+	struct hlist_node mod_hdr_hlist;
+
+	/* protects flows list */
+	spinlock_t flows_lock;
+	/* flows sharing the same mod_hdr entry */
+	struct list_head flows;
+
+	struct mod_hdr_key key;
+
+	u32 mod_hdr_id;
+
+#ifdef HAVE_REFCOUNT
+	refcount_t		refcnt;
+#else
+	atomic_t		refcnt;
+#endif
+	struct rcu_head rcu;
+};
+
+#define MLX5_MH_ACT_SZ MLX5_UN_SZ_BYTES(set_action_in_add_action_in_auto)
+
 static inline u32 hash_mod_hdr_info(struct mod_hdr_key *key)
 {
 	return jhash(key->actions,
@@ -241,7 +368,11 @@ mlx5e_mod_hdr_get(struct mlx5e_priv *pri
 		hash_for_each_possible_rcu(esw->offloads.mod_hdr_tbl, mh,
 					   mod_hdr_hlist, hash_key) {
 			if (!cmp_mod_hdr_info(&mh->key, key) &&
+#ifdef HAVE_REFCOUNT
 			    refcount_inc_not_zero(&mh->refcnt)) {
+#else
+			    atomic_inc_not_zero(&mh->refcnt)) {
+#endif
 				found = true;
 				break;
 			}
@@ -250,7 +381,11 @@ mlx5e_mod_hdr_get(struct mlx5e_priv *pri
 		hash_for_each_possible_rcu(priv->fs.tc.mod_hdr_tbl, mh,
 					   mod_hdr_hlist, hash_key) {
 			if (!cmp_mod_hdr_info(&mh->key, key) &&
+#ifdef HAVE_REFCOUNT
 			    refcount_inc_not_zero(&mh->refcnt)) {
+#else
+			    atomic_inc_not_zero(&mh->refcnt)) {
+#endif
 				found = true;
 				break;
 			}
@@ -288,7 +423,11 @@ mlx5e_mod_hdr_get_create(struct mlx5e_pr
 	mh->key.num_actions = num_actions;
 	spin_lock_init(&mh->flows_lock);
 	INIT_LIST_HEAD(&mh->flows);
+#ifdef HAVE_REFCOUNT
 	refcount_set(&mh->refcnt, 1);
+#else
+	atomic_set(&mh->refcnt, 1);
+#endif
 
 	err = mlx5_modify_header_alloc(priv->mdev, namespace,
 				       mh->key.num_actions,
@@ -342,7 +481,11 @@ static void mlx5e_mod_hdr_put(struct mlx
 			      struct mlx5e_mod_hdr_entry *mh,
 			      spinlock_t *tbl_lock)
 {
+#ifdef HAVE_REFCOUNT
 	if (refcount_dec_and_test(&mh->refcnt)) {
+#else
+	if (atomic_dec_and_test(&mh->refcnt)) {
+#endif
 		WARN_ON(!list_empty(&mh->flows));
 		mlx5_modify_header_dealloc(priv->mdev, mh->mod_hdr_id);
 		spin_lock(tbl_lock);
@@ -404,6 +547,7 @@ static void mlx5e_detach_mod_hdr(struct
 	mlx5e_mod_hdr_put(priv, flow->mh, tbl_lock);
 	flow->mh = NULL;
 }
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX */
 
 static
 struct mlx5_core_dev *mlx5e_hairpin_get_mdev(struct net *net, int ifindex)
@@ -675,7 +819,11 @@ static struct mlx5e_hairpin_entry *mlx5e
 	hash_for_each_possible_rcu(priv->fs.tc.hairpin_tbl, hpe,
 				   hairpin_hlist, hash_key) {
 		if (hpe->peer_vhca_id == peer_vhca_id && hpe->prio == prio &&
+#ifdef HAVE_REFCOUNT
 		    refcount_inc_not_zero(&hpe->refcnt)) {
+#else
+		    atomic_inc_not_zero(&hpe->refcnt)) {
+#endif
 			rcu_read_unlock();
 			return hpe;
 		}
@@ -708,7 +856,11 @@ mlx5e_hairpin_get_create(struct mlx5e_pr
 	INIT_LIST_HEAD(&hpe->flows);
 	hpe->peer_vhca_id = peer_id;
 	hpe->prio = match_prio;
+#ifdef HAVE_REFCOUNT
 	refcount_set(&hpe->refcnt, 1);
+#else
+	atomic_set(&hpe->refcnt, 1);
+#endif
 
 	params.log_data_size = 15;
 	params.log_data_size = min_t(u8, params.log_data_size,
@@ -771,7 +923,11 @@ static void mlx5e_hairpin_put(struct mlx
 			      struct mlx5e_hairpin_entry *hpe)
 {
 	/* no more hairpin flows for us, release the hairpin pair */
+#ifdef HAVE_REFCOUNT
 	if (refcount_dec_and_test(&hpe->refcnt)) {
+#else
+	if (atomic_dec_and_test(&hpe->refcnt)) {
+#endif
 		netdev_dbg(priv->netdev, "del hairpin: peer %s\n",
 			   hpe->hp->pair->peer_mdev->priv.name);
 
@@ -785,22 +941,31 @@ static void mlx5e_hairpin_put(struct mlx
 }
 
 #define UNKNOWN_MATCH_PRIO 8
-
 static int mlx5e_hairpin_get_prio(struct mlx5e_priv *priv,
-				  struct mlx5_flow_spec *spec, u8 *match_prio,
-				  struct netlink_ext_ack *extack)
+				  struct mlx5_flow_spec *spec, u8 *match_prio
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+				  ,struct netlink_ext_ack *extack)
+#else
+				)
+#endif
 {
 	void *headers_c, *headers_v;
 	u8 prio_val, prio_mask = 0;
 	bool vlan_present;
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	if (priv->dcbx_dp.trust_state != MLX5_QPTS_TRUST_PCP) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "only PCP trust state supported for hairpin");
+#else
+		netdev_warn(priv->netdev, "only PCP trust state supported for hairpin\n");
+#endif
 		return -EOPNOTSUPP;
 	}
 #endif
+#endif
 	headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, outer_headers);
 	headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value, outer_headers);
 
@@ -813,8 +978,12 @@ static int mlx5e_hairpin_get_prio(struct
 	if (!vlan_present || !prio_mask) {
 		prio_val = UNKNOWN_MATCH_PRIO;
 	} else if (prio_mask != 0x7) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "masked priority match not supported for hairpin");
+#else
+		netdev_warn(priv->netdev, "masked priority match not supported for hairpin\n");
+#endif
 		return -EOPNOTSUPP;
 	}
 
@@ -824,8 +993,12 @@ static int mlx5e_hairpin_get_prio(struct
 
 static int mlx5e_hairpin_flow_add(struct mlx5e_priv *priv,
 				  struct mlx5e_tc_flow *flow,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				  struct mlx5e_tc_flow_parse_attr *parse_attr,
 				  struct netlink_ext_ack *extack)
+#else
+				  struct mlx5e_tc_flow_parse_attr *parse_attr)
+#endif
 {
 	int peer_ifindex = parse_attr->mirred_ifindex;
 	struct mlx5_core_dev *peer_mdev;
@@ -836,13 +1009,20 @@ static int mlx5e_hairpin_flow_add(struct
 
 	peer_mdev = mlx5e_hairpin_get_mdev(dev_net(priv->netdev), peer_ifindex);
 	if (!MLX5_CAP_GEN(priv->mdev, hairpin) || !MLX5_CAP_GEN(peer_mdev, hairpin)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "hairpin is not supported");
+#else
+		netdev_warn(priv->netdev, "hairpin is not supported\n");
+#endif
 		return -EOPNOTSUPP;
 	}
 
 	peer_id = MLX5_CAP_GEN(peer_mdev, vhca_id);
-	err = mlx5e_hairpin_get_prio(priv, &parse_attr->spec, &match_prio,
-				     extack);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+	err = mlx5e_hairpin_get_prio(priv, &parse_attr->spec, &match_prio, extack);
+#else
+	err = mlx5e_hairpin_get_prio(priv, &parse_attr->spec, &match_prio);
+#endif
 	if (err)
 		return err;
 	hpe = mlx5e_hairpin_get_create(priv, peer_ifindex, peer_id, match_prio);
@@ -881,8 +1061,12 @@ static void mlx5e_hairpin_flow_del(struc
 static int
 mlx5e_tc_add_nic_flow(struct mlx5e_priv *priv,
 		      struct mlx5e_tc_flow_parse_attr *parse_attr,
-		      struct mlx5e_tc_flow *flow,
-		      struct netlink_ext_ack *extack)
+		      struct mlx5e_tc_flow *flow
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+		      ,struct netlink_ext_ack *extack)
+#else
+		      )
+#endif
 {
 	struct mlx5_nic_flow_attr *attr = flow->nic_attr;
 	struct mlx5_core_dev *dev = priv->mdev;
@@ -891,13 +1075,17 @@ mlx5e_tc_add_nic_flow(struct mlx5e_priv
 		.action = attr->action,
 		.flow_tag = attr->flow_tag,
 		.reformat_id = 0,
-		.flags    = FLOW_ACT_HAS_TAG | FLOW_ACT_NO_APPEND,
+		.flags	  = FLOW_ACT_HAS_TAG | FLOW_ACT_NO_APPEND,
 	};
 	struct mlx5_fc *counter = NULL;
 	int err, dest_ix = 0;
 
 	if (atomic_read(&flow->flags) & MLX5E_TC_FLOW_HAIRPIN) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		err = mlx5e_hairpin_flow_add(priv, flow, parse_attr, extack);
+#else
+		err = mlx5e_hairpin_flow_add(priv, flow, parse_attr);
+#endif
 		if (err)
 			return err;
 
@@ -926,12 +1114,14 @@ mlx5e_tc_add_nic_flow(struct mlx5e_priv
 		attr->counter = counter;
 	}
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR) {
 		err = mlx5e_attach_mod_hdr(priv, flow, parse_attr);
 		flow_act.modify_id = attr->mod_hdr_id;
 		if (err)
 			return err;
 	}
+#endif
 
 	mutex_lock(&priv->fs.tc.t_lock);
 	if (IS_ERR_OR_NULL(priv->fs.tc.t)) {
@@ -954,8 +1144,10 @@ mlx5e_tc_add_nic_flow(struct mlx5e_priv
 							    MLX5E_TC_FT_LEVEL, 0);
 		if (IS_ERR(priv->fs.tc.t)) {
 			mutex_unlock(&priv->fs.tc.t_lock);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Failed to create tc offload table\n");
+#endif
 			netdev_err(priv->netdev,
 				   "Failed to create tc offload table\n");
 			return PTR_ERR(priv->fs.tc.t);
@@ -993,13 +1185,16 @@ static void mlx5e_tc_del_nic_flow(struct
 	}
 	mutex_unlock(&priv->fs.tc.t_lock);
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
 		mlx5e_detach_mod_hdr(priv, flow);
+#endif
 
 	if (atomic_read(&flow->flags) & MLX5E_TC_FLOW_HAIRPIN)
 		mlx5e_hairpin_flow_del(priv, flow);
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void mlx5e_detach_encap(struct mlx5e_priv *priv,
 			       struct mlx5e_tc_flow *flow);
 
@@ -1007,8 +1202,11 @@ static int mlx5e_attach_encap(struct mlx
 			      struct ip_tunnel_info *tun_info,
 			      struct net_device *mirred_dev,
 			      struct net_device **encap_dev,
-			      struct mlx5e_tc_flow *flow,
-			      struct netlink_ext_ack *extack);
+			      struct mlx5e_tc_flow *flow
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+			      ,struct netlink_ext_ack *extack
+#endif
+				);
 
 static struct mlx5_flow_handle *
 mlx5e_tc_offload_fdb_rules(struct mlx5_eswitch *esw,
@@ -1032,13 +1230,21 @@ mlx5e_tc_offload_fdb_rules(struct mlx5_e
 
 	return rule;
 }
+#endif
 
 static void
 mlx5e_tc_unoffload_fdb_rules(struct mlx5_eswitch *esw,
 			     struct mlx5e_tc_flow *flow,
 			   struct mlx5_esw_flow_attr *attr)
 {
+#ifdef HAVE_ATOMIC_AND
 	atomic_and(~MLX5E_TC_FLOW_OFFLOADED, &flow->flags);
+#else
+	int flags = atomic_read(&flow->flags);
+
+	flags &= ~MLX5E_TC_FLOW_OFFLOADED;
+	atomic_set(&flow->flags, flags);
+#endif
 
 	if (attr->mirror_count)
 		mlx5_eswitch_del_fwd_rule(esw, flow->rule[1], attr);
@@ -1046,6 +1252,7 @@ mlx5e_tc_unoffload_fdb_rules(struct mlx5
 	mlx5_eswitch_del_offloaded_rule(esw, flow->rule[0], attr);
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static struct mlx5_flow_handle *
 mlx5e_tc_offload_to_slow_path(struct mlx5_eswitch *esw,
 			      struct mlx5e_tc_flow *flow,
@@ -1065,18 +1272,28 @@ mlx5e_tc_offload_to_slow_path(struct mlx
 
 	return rule;
 }
+#endif
 
 static void
 mlx5e_tc_unoffload_from_slow_path(struct mlx5_eswitch *esw,
 				  struct mlx5e_tc_flow *flow,
 				  struct mlx5_esw_flow_attr *slow_attr)
 {
+#ifndef HAVE_ATOMIC_AND
+	int flags;
+#endif
 	memcpy(slow_attr, flow->esw_attr, sizeof(*slow_attr));
 	slow_attr->action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
 	slow_attr->mirror_count = 0;
 	slow_attr->dest_chain = FDB_SLOW_PATH_CHAIN;
 	mlx5e_tc_unoffload_fdb_rules(esw, flow, slow_attr);
+#ifdef HAVE_ATOMIC_AND
 	atomic_and(~MLX5E_TC_FLOW_SLOW, &flow->flags);
+#else
+	flags = atomic_read(&flow->flags);
+	flags &= ~MLX5E_TC_FLOW_SLOW;
+	atomic_set(&flow->flags, flags);
+#endif
 }
 
 static void add_unready_flow(struct mlx5e_tc_flow *flow)
@@ -1093,47 +1310,86 @@ static void add_unready_flow(struct mlx5
 
 static void remove_unready_flow(struct mlx5e_tc_flow *flow)
 {
+#ifndef HAVE_ATOMIC_AND
+	int flags;
+#endif
 	list_del(&flow->unready);
+#ifdef HAVE_ATOMIC_AND
 	atomic_and(~MLX5E_TC_FLOW_NOT_READY, &flow->flags);
+#else
+	flags = atomic_read(&flow->flags);
+	flags &= ~MLX5E_TC_FLOW_NOT_READY;
+	atomic_set(&flow->flags, flags);
+#endif
 }
 
 static int
 mlx5e_tc_add_fdb_flow(struct mlx5e_priv *priv,
 		      struct mlx5e_tc_flow_parse_attr *parse_attr,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		      struct mlx5e_tc_flow *flow,
 		      struct netlink_ext_ack *extack)
+#else
+		      struct mlx5e_tc_flow *flow)
+#endif
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && defined (HAVE_IS_TCF_GACT_GOTO_CHAIN)
 	u32 max_chain = mlx5_eswitch_get_chain_range(esw);
-	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 	u16 max_prio = mlx5_eswitch_get_prio_range(esw);
+#endif
+	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct net_device *out_dev, *encap_dev = NULL;
+#endif
 	struct mlx5_fc *counter = NULL;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct mlx5e_rep_priv *rpriv;
 	struct mlx5e_priv *out_priv;
 	int err = 0, encap_err = 0;
-
+#else
+	int err;
+#endif
+#ifdef HAVE_TCF_TUNNEL_INFO
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && defined (HAVE_IS_TCF_GACT_GOTO_CHAIN)
 	if (!mlx5_eswitch_prios_supported(esw) && attr->prio != 1) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG(extack, "E-switch priorities unsupported, upgrade FW");
+#else
+		netdev_err(priv->netdev,"E-switch priorities unsupported, upgrade\n");
+#endif
 		return -EOPNOTSUPP;
 	}
 
 	if (attr->chain > max_chain) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG(extack, "Requested chain is out of supported range");
+#else
+		netdev_err(priv->netdev,"Requested chain is out of supported range\n");
+#endif
 		return -EOPNOTSUPP;
 	}
 
 	if (attr->prio > max_prio) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG(extack, "Requested priority is out of supported range");
+#else
+		netdev_err(priv->netdev,"Requested priority is out of supported range\n");
+#endif
 		return -EOPNOTSUPP;
 	}
+#endif /* HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON */
 
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT) {
 		out_dev = __dev_get_by_index(dev_net(priv->netdev),
 					     attr->parse_attr->mirred_ifindex);
 		encap_err = mlx5e_attach_encap(priv, &parse_attr->tun_info,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 					       out_dev, &encap_dev, flow,
 					       extack);
+#else
+					       out_dev, &encap_dev, flow);
+#endif
 		if (encap_err && encap_err != -EAGAIN)
 			return encap_err;
 
@@ -1142,11 +1398,13 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		attr->out_rep[attr->out_count] = rpriv->rep;
 		attr->out_mdev[attr->out_count++] = out_priv->mdev;
 	}
+#endif
 
 	err = mlx5_eswitch_add_vlan_action(esw, attr);
 	if (err)
 		return err;
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR) {
 		err = mlx5e_attach_mod_hdr(priv, flow, parse_attr);
 		kfree(parse_attr->mod_hdr_actions);
@@ -1154,15 +1412,17 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		if (err)
 			return err;
 	}
+#endif
 
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_COUNT) {
-		counter = mlx5_fc_create(esw->dev, true);
+		counter = mlx5_fc_create(attr->counter_dev, true);
 		if (IS_ERR(counter))
 			return PTR_ERR(counter);
 
 		attr->counter = counter;
 	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	/* we get here if (1) there's no error or when
 	 * (2) there's an encap action and we're on -EAGAIN (no valid neigh)
 	 */
@@ -1179,6 +1439,7 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 		return PTR_ERR(flow->rule[0]);
 	else
 		mlx5e_set_flow_flag_mb_before(flow, MLX5E_TC_FLOW_OFFLOADED);
+#endif
 
 	return 0;
 }
@@ -1208,21 +1469,38 @@ static void mlx5e_tc_del_fdb_flow(struct
 
 	mlx5_eswitch_del_vlan_action(esw, attr);
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT)
 		mlx5e_detach_encap(priv, flow);
+#endif
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
 		mlx5e_detach_mod_hdr(priv, flow);
+#endif
 
 	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_COUNT)
 		mlx5_fc_destroy(esw->dev, attr->counter);
 
+#if defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX) && defined(HAVE_TCF_TUNNEL_INFO)
 	if (attr->parse_attr) {
 		kfree(attr->parse_attr->mod_hdr_actions);
 		kvfree(attr->parse_attr);
 	}
+#endif
+}
+
+#if defined(HAVE_TCF_TUNNEL_INFO) || defined(HAVE_TC_CLSFLOWER_STATS)
+static struct mlx5_fc *mlx5e_tc_get_counter(struct mlx5e_tc_flow *flow)
+{
+	if (atomic_read(&flow->flags) & MLX5E_TC_FLOW_ESWITCH)
+		return flow->esw_attr->counter;
+	else
+		return flow->nic_attr->counter;
 }
+#endif
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void mlx5e_put_flow_list(struct mlx5e_priv *priv,
 				struct list_head *flow_list)
 {
@@ -1336,14 +1614,6 @@ void mlx5e_tc_encap_flows_del(struct mlx
 	mlx5e_put_flow_list(priv, &deleted_flows);
 }
 
-static struct mlx5_fc *mlx5e_tc_get_counter(struct mlx5e_tc_flow *flow)
-{
-	if (mlx5e_is_eswitch_flow(flow))
-		return flow->esw_attr->counter;
-	else
-		return flow->nic_attr->counter;
-}
-
 static struct mlx5e_tc_flow *
 mlx5e_get_next_encap_flow(struct mlx5e_encap_entry *e,
 			  struct mlx5e_tc_flow *flow)
@@ -1445,9 +1715,12 @@ void mlx5e_tc_update_neigh_used_value(st
 
 	if (m_neigh->family == AF_INET)
 		tbl = &arp_tbl;
-#if IS_ENABLED(CONFIG_IPV6)
-	else if (m_neigh->family == AF_INET6)
-		tbl = &nd_tbl;
+#if defined(__IPV6_SUPPORT__) && IS_ENABLED(CONFIG_IPV6)
+	else if (m_neigh->family == AF_INET6) {
+		if (!ipv6_stub || !ipv6_stub->nd_tbl)
+			return;
+		tbl = ipv6_stub->nd_tbl;
+	}
 #endif
 	else
 		return;
@@ -1490,7 +1763,11 @@ void mlx5e_encap_put(struct mlx5e_priv *
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 
+#ifdef HAVE_REFCOUNT
 	if (refcount_dec_and_test(&e->refcnt)) {
+#else
+	if (atomic_dec_and_test(&e->refcnt)) {
+#endif
 		WARN_ON(!list_empty(&e->flows));
 		/* encap can be deleted before attachment to dev if error
 		 * happens during encap initialization
@@ -1582,9 +1859,13 @@ static int mlx5e_tc_update_and_init_done
 
 	return err;
 }
+#endif
 
 static void __mlx5e_tc_del_fdb_peer_flow(struct mlx5e_tc_flow *flow)
 {
+#ifndef HAVE_ATOMIC_AND
+	int flags;
+#endif
 	struct mlx5_eswitch *esw = flow->priv->mdev->priv.eswitch;
 
 	if (!(atomic_read(&flow->flags) & MLX5E_TC_FLOW_ESWITCH) ||
@@ -1595,7 +1876,13 @@ static void __mlx5e_tc_del_fdb_peer_flow
 	list_del(&flow->peer);
 	mutex_unlock(&esw->offloads.peer_mutex);
 
+#ifdef HAVE_ATOMIC_AND
 	atomic_and(~MLX5E_TC_FLOW_DUP, &flow->flags);
+#else
+	flags = atomic_read(&flow->flags);
+	flags &= ~MLX5E_TC_FLOW_DUP;
+	atomic_set(&flow->flags, flags);
+#endif
 
 	mlx5e_tc_del_fdb_flow(flow->peer_flow->priv, flow->peer_flow);
 	kvfree(flow->peer_flow);
@@ -1627,6 +1914,7 @@ static void mlx5e_tc_del_flow(struct mlx
 	}
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void parse_vxlan_attr(struct mlx5_flow_spec *spec,
 			     struct tc_cls_flower_offload *f)
 {
@@ -1663,7 +1951,9 @@ static int parse_tunnel_attr(struct mlx5
 			     struct tc_cls_flower_offload *f,
 			     u8 *match_level)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				       outer_headers);
 	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
@@ -1692,8 +1982,10 @@ static int parse_tunnel_attr(struct mlx5
 		    MLX5_CAP_ESW(priv->mdev, vxlan_encap_decap))
 			parse_vxlan_attr(spec, f);
 		else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "port isn't an offloaded vxlan udp dport");
+#endif
 			netdev_warn(priv->netdev,
 				    "%d isn't an offloaded vxlan udp dport\n", be16_to_cpu(key->dst));
 			return -EOPNOTSUPP;
@@ -1711,8 +2003,10 @@ static int parse_tunnel_attr(struct mlx5
 			 udp_sport, ntohs(key->src));
 	} else { /* udp dst port must be given */
 vxlan_match_offload_err:
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "IP tunnel decap offload supported only for vxlan, must set UDP dport");
+#endif
 		netdev_warn(priv->netdev,
 			    "IP tunnel decap offload supported only for vxlan, must set UDP dport\n");
 		return -EOPNOTSUPP;
@@ -1771,6 +2065,7 @@ vxlan_match_offload_err:
 		MLX5_SET(fte_match_set_lyr_2_4, headers_v, ethertype, ETH_P_IPV6);
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_IP
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_ENC_IP)) {
 		struct flow_dissector_key_ip *key =
 			skb_flow_dissector_target(f->dissector,
@@ -1789,18 +2084,20 @@ vxlan_match_offload_err:
 
 		MLX5_SET(fte_match_set_lyr_2_4, headers_c, ttl_hoplimit, mask->ttl);
 		MLX5_SET(fte_match_set_lyr_2_4, headers_v, ttl_hoplimit, key->ttl);
-
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		if (mask->ttl &&
 		    !MLX5_CAP_ESW_FLOWTABLE_FDB
 			(priv->mdev,
-			 ft_field_support.outer_ipv4_ttl)) {
+			 ft_field_support.outer_ipv4_ttl))
+			 {
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Matching on TTL is not supported");
 			return -EOPNOTSUPP;
 		}
-
+#endif
 	}
 
+#endif
 	/* Enforce DMAC when offloading incoming tunneled flows.
 	 * Flow counters require a match on the DMAC.
 	 */
@@ -1815,21 +2112,26 @@ vxlan_match_offload_err:
 
 	return 0;
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static int __parse_cls_flower(struct mlx5e_priv *priv,
 			      struct mlx5_flow_spec *spec,
 			      struct tc_cls_flower_offload *f,
 			      u8 *match_level, u8 *tunnel_match_level)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				       outer_headers);
 	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
 				       outer_headers);
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN
 	void *misc_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				    misc_parameters);
 	void *misc_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
 				    misc_parameters);
+#endif
 	u16 addr_type = 0;
 	u8 ip_proto = 0;
 
@@ -1839,25 +2141,43 @@ static int __parse_cls_flower(struct mlx
 	    ~(BIT(FLOW_DISSECTOR_KEY_CONTROL) |
 	      BIT(FLOW_DISSECTOR_KEY_BASIC) |
 	      BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN
 	      BIT(FLOW_DISSECTOR_KEY_VLAN) |
+#else
+	      BIT(FLOW_DISSECTOR_KEY_VLANID) |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN
 	      BIT(FLOW_DISSECTOR_KEY_CVLAN) |
+#endif
 	      BIT(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
+#ifdef HAVE_TCF_TUNNEL_INFO
 	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_KEYID) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_PORTS)	|
 	      BIT(FLOW_DISSECTOR_KEY_ENC_CONTROL) |
+#else
+	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_TCP
 	      BIT(FLOW_DISSECTOR_KEY_TCP) |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_IP
 	      BIT(FLOW_DISSECTOR_KEY_IP)  |
+#endif
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_IP
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IP))) {
-		NL_SET_ERR_MSG_MOD(extack, "Unsupported key");
+#else
+	      0)) {
+#endif
 		netdev_warn(priv->netdev, "Unsupported key used: 0x%x\n",
 			    f->dissector->used_keys);
 		return -EOPNOTSUPP;
 	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	if ((dissector_uses_key(f->dissector,
 				FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) ||
 	     dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_ENC_KEYID) ||
@@ -1885,6 +2205,7 @@ static int __parse_cls_flower(struct mlx
 		headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
 					 inner_headers);
 	}
+#endif
 
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_BASIC)) {
 		struct flow_dissector_key_basic *key =
@@ -1904,6 +2225,7 @@ static int __parse_cls_flower(struct mlx
 			*match_level = MLX5_MATCH_L2;
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_VLAN)) {
 		struct flow_dissector_key_vlan *key =
 			skb_flow_dissector_target(f->dissector,
@@ -1913,6 +2235,7 @@ static int __parse_cls_flower(struct mlx
 			skb_flow_dissector_target(f->dissector,
 						  FLOW_DISSECTOR_KEY_VLAN,
 						  f->mask);
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN
 		if (mask->vlan_id || mask->vlan_priority || mask->vlan_tpid) {
 			if (key->vlan_tpid == htons(ETH_P_8021AD)) {
 				MLX5_SET(fte_match_set_lyr_2_4, headers_c,
@@ -1925,7 +2248,11 @@ static int __parse_cls_flower(struct mlx
 				MLX5_SET(fte_match_set_lyr_2_4, headers_v,
 					 cvlan_tag, 1);
 			}
-
+#else
+			if (mask->vlan_id || mask->vlan_priority) {
+				MLX5_SET(fte_match_set_lyr_2_4, headers_c, cvlan_tag, 1);
+				MLX5_SET(fte_match_set_lyr_2_4, headers_v, cvlan_tag, 1);
+#endif /* HAVE_FLOW_DISSECTOR_KEY_CVLAN */
 			MLX5_SET(fte_match_set_lyr_2_4, headers_c, first_vid, mask->vlan_id);
 			MLX5_SET(fte_match_set_lyr_2_4, headers_v, first_vid, key->vlan_id);
 
@@ -1934,12 +2261,32 @@ static int __parse_cls_flower(struct mlx
 
 			*match_level = MLX5_MATCH_L2;
 		}
+#else
+	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_VLANID)) {
+		struct flow_dissector_key_tags *key =
+			skb_flow_dissector_target(f->dissector,
+						  FLOW_DISSECTOR_KEY_VLANID,
+						  f->key);
+		struct flow_dissector_key_tags *mask =
+			skb_flow_dissector_target(f->dissector,
+						  FLOW_DISSECTOR_KEY_VLANID,
+						  f->mask);
+		if (mask->vlan_id) {
+			MLX5_SET(fte_match_set_lyr_2_4, headers_c, cvlan_tag, 1);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_v, cvlan_tag, 1);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_c, first_vid, mask->vlan_id);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_v, first_vid, key->vlan_id);
+
+			*match_level = MLX5_MATCH_L2;
+		}
+#endif
 	} else if (*match_level != MLX5_MATCH_NONE) {
 		MLX5_SET(fte_match_set_lyr_2_4, headers_c, svlan_tag, 1);
 		MLX5_SET(fte_match_set_lyr_2_4, headers_c, cvlan_tag, 1);
 		*match_level = MLX5_MATCH_L2;
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_CVLAN 
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_CVLAN)) {
 		struct flow_dissector_key_vlan *key =
 			skb_flow_dissector_target(f->dissector,
@@ -1974,7 +2321,7 @@ static int __parse_cls_flower(struct mlx
 			*match_level = MLX5_MATCH_L2;
 		}
 	}
-
+#endif /* HAVE_FLOW_DISSECTOR_KEY_CVLAN */
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {
 		struct flow_dissector_key_eth_addrs *key =
 			skb_flow_dissector_target(f->dissector,
@@ -2027,7 +2374,7 @@ static int __parse_cls_flower(struct mlx
 			/* the HW doesn't need L3 inline to match on frag=no */
 			if (!(key->flags & FLOW_DIS_IS_FRAGMENT))
 				*match_level = MLX5_MATCH_L2;
-	/* ***  L2 attributes parsing up to here *** */
+	/* ***	L2 attributes parsing up to here *** */
 			else
 				*match_level = MLX5_MATCH_L3;
 		}
@@ -2109,6 +2456,7 @@ static int __parse_cls_flower(struct mlx
 			*match_level = MLX5_MATCH_L3;
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_IP
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_IP)) {
 		struct flow_dissector_key_ip *key =
 			skb_flow_dissector_target(f->dissector,
@@ -2131,16 +2479,19 @@ static int __parse_cls_flower(struct mlx
 		if (mask->ttl &&
 		    !MLX5_CAP_ESW_FLOWTABLE_FDB(priv->mdev,
 						ft_field_support.outer_ipv4_ttl)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Matching on TTL is not supported");
+#endif
 			return -EOPNOTSUPP;
 		}
 
 		if (mask->tos || mask->ttl)
 			*match_level = MLX5_MATCH_L3;
 	}
+#endif
 
-	/* ***  L3 attributes parsing up to here *** */
+	/* ***	L3 attributes parsing up to here *** */
 
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_PORTS)) {
 		struct flow_dissector_key_ports *key =
@@ -2176,8 +2527,10 @@ static int __parse_cls_flower(struct mlx
 				 udp_dport, ntohs(key->dst));
 			break;
 		default:
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Only UDP and TCP transports are supported for L4 matching");
+#endif
 			netdev_err(priv->netdev,
 				   "Only UDP and TCP transport are supported\n");
 			return -EINVAL;
@@ -2187,6 +2540,7 @@ static int __parse_cls_flower(struct mlx
 			*match_level = MLX5_MATCH_L4;
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_TCP
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_TCP)) {
 		struct flow_dissector_key_tcp *key =
 			skb_flow_dissector_target(f->dissector,
@@ -2205,6 +2559,7 @@ static int __parse_cls_flower(struct mlx
 		if (mask->flags)
 			*match_level = MLX5_MATCH_L4;
 	}
+#endif
 
 	return 0;
 }
@@ -2214,7 +2569,9 @@ static int parse_cls_flower(struct mlx5e
 			    struct mlx5_flow_spec *spec,
 			    struct tc_cls_flower_offload *f)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5_core_dev *dev = priv->mdev;
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
@@ -2231,8 +2588,10 @@ static int parse_cls_flower(struct mlx5e
 		if (rep->vport != MLX5_VPORT_UPLINK &&
 		    (esw->offloads.inline_mode != MLX5_INLINE_MODE_NONE &&
 		    esw->offloads.inline_mode < match_level)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Flow is not offloaded due to min inline setting");
+#endif
 			netdev_warn(priv->netdev,
 				    "Flow is not offloaded due to min inline setting, required %d actual %d\n",
 				    match_level, esw->offloads.inline_mode);
@@ -2240,7 +2599,7 @@ static int parse_cls_flower(struct mlx5e
 		}
 	}
 
-	if (is_eswitch_flow)
+	if (is_eswitch_flow) {
 		flow->esw_attr->match_level = match_level;
 		flow->esw_attr->tunnel_match_level = tunnel_match_level;
 	} else {
@@ -2250,6 +2609,7 @@ static int parse_cls_flower(struct mlx5e
 	return err;
 }
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 struct pedit_headers {
 	struct ethhdr  eth;
 	struct vlan_hdr vlan;
@@ -2311,8 +2671,8 @@ static struct mlx5_fields fields[] = {
 	OFFLOAD(FIRST_VID,  2, vlan.h_vlan_TCI, 0),
 
 	OFFLOAD(IP_TTL, 1, ip4.ttl,   0),
-	OFFLOAD(SIPV4,  4, ip4.saddr, 0),
-	OFFLOAD(DIPV4,  4, ip4.daddr, 0),
+	OFFLOAD(SIPV4,	4, ip4.saddr, 0),
+	OFFLOAD(DIPV4,	4, ip4.daddr, 0),
 
 	OFFLOAD(SIPV6_127_96, 4, ip6.saddr.s6_addr32[0], 0),
 	OFFLOAD(SIPV6_95_64,  4, ip6.saddr.s6_addr32[1], 0),
@@ -2338,8 +2698,11 @@ static struct mlx5_fields fields[] = {
  */
 static int offload_pedit_fields(struct pedit_headers *masks,
 				struct pedit_headers *vals,
-				struct mlx5e_tc_flow_parse_attr *parse_attr,
-				struct netlink_ext_ack *extack)
+				struct mlx5e_tc_flow_parse_attr *parse_attr
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+				,struct netlink_ext_ack *extack
+#endif
+				)
 {
 	struct pedit_headers *set_masks, *add_masks, *set_vals, *add_vals;
 	int i, action_size, nactions, max_actions, first, last, next_z;
@@ -2380,15 +2743,19 @@ static int offload_pedit_fields(struct p
 			continue;
 
 		if (s_mask && a_mask) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "can't set and add to the same HW field");
+#endif
 			printk(KERN_WARNING "mlx5: can't set and add to the same HW field (%x)\n", f->field);
 			return -EOPNOTSUPP;
 		}
 
 		if (nactions == max_actions) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "too many pedit actions, can't offload");
+#endif
 			printk(KERN_WARNING "mlx5: parsed %d pedit actions, can't do more\n", nactions);
 			return -EOPNOTSUPP;
 		}
@@ -2421,8 +2788,10 @@ static int offload_pedit_fields(struct p
 		next_z = find_next_zero_bit(&mask, field_bsize, first);
 		last  = find_last_bit(&mask, field_bsize);
 		if (first < next_z && next_z < last) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "rewrite of few sub-fields isn't supported");
+#endif
 			printk(KERN_WARNING "mlx5: rewrite of few sub-fields (mask %lx) isn't offloaded\n",
 			       mask);
 			return -EOPNOTSUPP;
@@ -2481,8 +2850,11 @@ static const struct pedit_headers zero_m
 
 static int parse_tc_pedit_action(struct mlx5e_priv *priv,
 				 const struct tc_action *a, int namespace,
-				 struct mlx5e_tc_flow_parse_attr *parse_attr,
-				 struct netlink_ext_ack *extack)
+				 struct mlx5e_tc_flow_parse_attr *parse_attr
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+				 ,struct netlink_ext_ack *extack
+#endif
+				 )
 {
 	struct pedit_headers masks[__PEDIT_CMD_MAX], vals[__PEDIT_CMD_MAX], *cmd_masks;
 	int nkeys, i, err = -EOPNOTSUPP;
@@ -2500,13 +2872,21 @@ static int parse_tc_pedit_action(struct
 		err = -EOPNOTSUPP; /* can't be all optimistic */
 
 		if (htype == TCA_PEDIT_KEY_EX_HDR_TYPE_NETWORK) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "legacy pedit isn't offloaded");
+#else
+			netdev_warn(priv->netdev, "legacy pedit isn't offloaded\n");
+#endif
 			goto out_err;
 		}
 
 		if (cmd != TCA_PEDIT_KEY_EX_CMD_SET && cmd != TCA_PEDIT_KEY_EX_CMD_ADD) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack, "pedit cmd isn't offloaded");
+#else
+			netdev_warn(priv->netdev, "pedit cmd %d isn't offloaded\n", cmd);
+#endif
 			goto out_err;
 		}
 
@@ -2525,15 +2905,21 @@ static int parse_tc_pedit_action(struct
 			goto out_err;
 	}
 
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	err = offload_pedit_fields(masks, vals, parse_attr, extack);
+#else
+	err = offload_pedit_fields(masks, vals, parse_attr);
+#endif
 	if (err < 0)
 		goto out_dealloc_parsed_actions;
 
 	for (cmd = 0; cmd < __PEDIT_CMD_MAX; cmd++) {
 		cmd_masks = &masks[cmd];
 		if (memcmp(cmd_masks, &zero_masks, sizeof(zero_masks))) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "attempt to offload an unsupported field");
+#endif
 			netdev_warn(priv->netdev, "attempt to offload an unsupported field (cmd %d)\n", cmd);
 			print_hex_dump(KERN_WARNING, "mask: ", DUMP_PREFIX_ADDRESS,
 				       16, 1, cmd_masks, sizeof(zero_masks), true);
@@ -2550,27 +2936,36 @@ out_dealloc_parsed_actions:
 out_err:
 	return err;
 }
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX */
 
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 static bool csum_offload_supported(struct mlx5e_priv *priv,
 				   u32 action,
-				   u32 update_flags,
-				   struct netlink_ext_ack *extack)
+				   u32 update_flags
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+				   ,struct netlink_ext_ack *extack
+#endif
+				   )
 {
 	u32 prot_flags = TCA_CSUM_UPDATE_FLAG_IPV4HDR | TCA_CSUM_UPDATE_FLAG_TCP |
 			 TCA_CSUM_UPDATE_FLAG_UDP;
 
 	/*  The HW recalcs checksums only if re-writing headers */
 	if (!(action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "TC csum action is only offloaded with pedit");
+#endif
 		netdev_warn(priv->netdev,
 			    "TC csum action is only offloaded with pedit\n");
 		return false;
 	}
 
 	if (update_flags & ~prot_flags) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "can't offload TC csum action for some header/s");
+#endif
 		netdev_warn(priv->netdev,
 			    "can't offload TC csum action for some header/s - flags %#x\n",
 			    update_flags);
@@ -2579,7 +2974,9 @@ static bool csum_offload_supported(struc
 
 	return true;
 }
+#endif
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 struct ip_ttl_word {
 	__u8	ttl;
 	__u8	protocol;
@@ -2631,16 +3028,22 @@ static bool is_action_keys_supported(con
 
 static bool modify_header_match_supported(struct mlx5_flow_spec *spec,
 					  struct tcf_exts *exts,
-					  u32 match_actions,
-					  struct netlink_ext_ack *extack)
+					  u32 match_actions
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+					  ,struct netlink_ext_ack *extack
+#endif
+					  )
 {
 	const struct tc_action *a;
 	bool modify_ip_header;
-	LIST_HEAD(actions);
 	void *headers_v;
 	u16 ethertype;
 	u8 ip_proto;
+#ifdef tcf_exts_for_each_action
 	int i;
+#else
+	LIST_HEAD(actions);
+#endif
 
 	if (match_actions & MLX5_FLOW_CONTEXT_ACTION_DECAP)
 		headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value, inner_headers);
@@ -2654,7 +3057,16 @@ static bool modify_header_match_supporte
 		goto out_ok;
 
 	modify_ip_header = false;
+#ifdef HAVE_TCF_EXTS_TO_LIST
+	tcf_exts_to_list(exts, &actions);
+	list_for_each_entry(a, &actions, list) {
+#else
+#ifdef tcf_exts_for_each_action
 	tcf_exts_for_each_action(i, a, exts) {
+#else
+	tc_for_each_action(a, exts) {
+#endif
+#endif
 		if (!is_tcf_pedit(a))
 			continue;
 
@@ -2664,8 +3076,10 @@ static bool modify_header_match_supporte
 	ip_proto = MLX5_GET(fte_match_set_lyr_2_4, headers_v, ip_protocol);
 	if (modify_ip_header && ip_proto != IPPROTO_TCP &&
 	    ip_proto != IPPROTO_UDP && ip_proto != IPPROTO_ICMP) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "can't offload re-write of non TCP/UDP");
+#endif
 		pr_info("can't offload re-write of ip proto %d\n", ip_proto);
 		return false;
 	}
@@ -2673,12 +3087,17 @@ static bool modify_header_match_supporte
 out_ok:
 	return true;
 }
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX */
 
+#if defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX) || defined(HAVE_TCF_TUNNEL_INFO)
 static bool actions_match_supported(struct mlx5e_priv *priv,
 				    struct tcf_exts *exts,
 				    struct mlx5e_tc_flow_parse_attr *parse_attr,
-				    struct mlx5e_tc_flow *flow,
-				    struct netlink_ext_ack *extack)
+				    struct mlx5e_tc_flow *flow
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+				    ,struct netlink_ext_ack *extack
+#endif
+				    )
 {
 	u32 actions;
 
@@ -2694,12 +3113,18 @@ static bool actions_match_supported(stru
 		return false;
 #endif
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	if (actions & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
-		return modify_header_match_supported(&parse_attr->spec, exts,
-						     actions, extack);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+		return modify_header_match_supported(&parse_attr->spec, exts, actions, extack);
+#else
+		return modify_header_match_supported(&parse_attr->spec, exts, actions);
+#endif
+#endif
 
 	return true;
 }
+#endif
 
 static bool same_hw_devs(struct mlx5e_priv *priv, struct mlx5e_priv *peer_priv)
 {
@@ -2717,21 +3142,43 @@ static bool same_hw_devs(struct mlx5e_pr
 
 static int parse_tc_nic_actions(struct mlx5e_priv *priv, struct tcf_exts *exts,
 				struct mlx5e_tc_flow_parse_attr *parse_attr,
-				struct mlx5e_tc_flow *flow,
-				struct netlink_ext_ack *extack)
+				struct mlx5e_tc_flow *flow
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+				,struct netlink_ext_ack *extack
+#endif
+				)
 {
 	struct mlx5_nic_flow_attr *attr = flow->nic_attr;
 	const struct tc_action *a;
+#ifdef tcf_exts_for_each_action
+	int i;
+#else
 	LIST_HEAD(actions);
+#endif
 	u32 action = 0;
-	int err, i;
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
+	int err;
+#endif
 
+#ifdef HAVE_TCF_EXTS_HAS_ACTIONS
 	if (!tcf_exts_has_actions(exts))
+#else
+	if (tc_no_actions(exts))
+#endif
 		return -EINVAL;
 
 	attr->flow_tag = MLX5_FS_DEFAULT_FLOW_TAG;
-
+#ifdef HAVE_TCF_EXTS_TO_LIST
+	tcf_exts_to_list(exts, &actions);
+	list_for_each_entry(a, &actions, list) {
+#else
+#ifdef tcf_exts_for_each_action
 	tcf_exts_for_each_action(i, a, exts) {
+#else
+	tc_for_each_action(a, exts) {
+#endif
+#endif
+#ifdef HAVE_IS_TCF_GACT_SHOT
 		if (is_tcf_gact_shot(a)) {
 			action |= MLX5_FLOW_CONTEXT_ACTION_DROP;
 			if (MLX5_CAP_FLOWTABLE(priv->mdev,
@@ -2739,10 +3186,16 @@ static int parse_tc_nic_actions(struct m
 				action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			continue;
 		}
+#endif
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 		if (is_tcf_pedit(a)) {
 			err = parse_tc_pedit_action(priv, a, MLX5_FLOW_NAMESPACE_KERNEL,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 						    parse_attr, extack);
+#else
+						    parse_attr);
+#endif
 			if (err)
 				return err;
 
@@ -2750,18 +3203,30 @@ static int parse_tc_nic_actions(struct m
 				  MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
 			continue;
 		}
+#endif
 
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 		if (is_tcf_csum(a)) {
 			if (csum_offload_supported(priv, action,
-						   tcf_csum_update_flags(a),
-						   extack))
+						   tcf_csum_update_flags(a)
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+						   ,extack))
+#else
+						   ))
+#endif
 				continue;
 
 			return -EOPNOTSUPP;
 		}
+#endif
 
 		if (is_tcf_mirred_egress_redirect(a)) {
+#ifdef HAVE_TCF_MIRRED_DEV
 			struct net_device *peer_dev = tcf_mirred_dev(a);
+#else
+			int ifindex = tcf_mirred_ifindex(a);
+			struct net_device *peer_dev = __dev_get_by_index(dev_net(priv->netdev), ifindex);
+#endif
 
 			if (priv->netdev->netdev_ops == peer_dev->netdev_ops &&
 			    same_hw_devs(priv, netdev_priv(peer_dev))) {
@@ -2770,8 +3235,10 @@ static int parse_tc_nic_actions(struct m
 				action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 					  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			} else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "device is not on same HW, can't offload");
+#endif
 				netdev_warn(priv->netdev, "device %s not on same HW, can't offload\n",
 					    peer_dev->name);
 				return -EINVAL;
@@ -2779,12 +3246,17 @@ static int parse_tc_nic_actions(struct m
 			continue;
 		}
 
+#ifdef HAVE_IS_TCF_SKBEDIT_MARK
 		if (is_tcf_skbedit_mark(a)) {
 			u32 mark = tcf_skbedit_mark(a);
 
 			if (mark & ~MLX5E_TC_FLOW_ID_MASK) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "Bad flow mark - only 16 bit is supported");
+#else
+				netdev_warn(priv->netdev, "Bad flow mark - only 16 bit is supported: 0x%x\n",mark);
+#endif
 				return -EINVAL;
 			}
 
@@ -2794,25 +3266,37 @@ static int parse_tc_nic_actions(struct m
 		}
 
 		return -EINVAL;
+#endif
 	}
 
 	attr->action = action;
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	if (!actions_match_supported(priv, exts, parse_attr, flow, extack))
+#else
+	if (!actions_match_supported(priv, exts, parse_attr, flow))
+#endif
 		return -EOPNOTSUPP;
+#endif
 
 	return 0;
 }
 
 static struct net_device *mlx5_upper_lag_dev_get(struct net_device *uplink_dev)
 {
-        struct net_device *upper = netdev_master_upper_dev_get(uplink_dev);
+	struct net_device *upper = netdev_master_upper_dev_get(uplink_dev);
 
-        if (upper && netif_is_lag_master(upper))
-                return upper;
-        else
-                return NULL;
+#if defined(HAVE_LAG_TX_TYPE) || defined(MLX_USE_LAG_COMPAT)
+	if (upper && netif_is_lag_master(upper))
+#else
+	if (upper && netif_is_bond_master(upper))
+#endif
+		return upper;
+	else
+		return NULL;
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static inline int cmp_encap_info(struct ip_tunnel_key *a,
 				 struct ip_tunnel_key *b)
 {
@@ -2868,7 +3352,7 @@ static int mlx5e_route_lookup_ipv4(struc
 				   struct neighbour **out_n,
 				   u8 *out_ttl)
 {
- 	struct neighbour *n = NULL;
+	struct neighbour *n = NULL;
 	struct rtable *rt;
 
 #if IS_ENABLED(CONFIG_INET)
@@ -2908,6 +3392,7 @@ static int mlx5e_route_lookup_ipv4(struc
 	*out_n = n;
 	return 0;
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static bool is_merged_eswitch_dev(struct mlx5e_priv *priv,
 				  struct net_device *peer_netdev)
@@ -2922,6 +3407,8 @@ static bool is_merged_eswitch_dev(struct
 		same_hw_devs(priv, peer_priv));
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
+#ifdef __IPV6_SUPPORT__
 static int mlx5e_route_lookup_ipv6(struct mlx5e_priv *priv,
 				   struct net_device *mirred_dev,
 				   struct net_device **out_dev,
@@ -2960,6 +3447,7 @@ static int mlx5e_route_lookup_ipv6(struc
 	*out_n = n;
 	return 0;
 }
+#endif
 
 static char *gen_eth_tnl_hdr(char *buf, struct net_device *dev,
 			     unsigned char h_dest[ETH_ALEN],
@@ -3019,6 +3507,7 @@ static void gen_vxlan_header_ipv4(struct
 	vxh->vx_vni = vxlan_vni_field(vx_vni);
 }
 
+#ifdef __IPV6_SUPPORT__
 static void gen_vxlan_header_ipv6(struct net_device *out_dev,
 				  char buf[], int encap_size,
 				  unsigned char h_dest[ETH_ALEN],
@@ -3038,7 +3527,7 @@ static void gen_vxlan_header_ipv6(struct
 						 ETH_P_IPV6);
 	ip6_flow_hdr(ip6h, tos, 0);
 	/* the HW fills up ipv6 payload len */
-	ip6h->nexthdr     = IPPROTO_UDP;
+	ip6h->nexthdr	  = IPPROTO_UDP;
 	ip6h->hop_limit   = ttl;
 	ip6h->daddr	  = *daddr;
 	ip6h->saddr	  = *saddr;
@@ -3050,6 +3539,7 @@ static void gen_vxlan_header_ipv6(struct
 	vxh->vx_flags = VXLAN_HF_VNI;
 	vxh->vx_vni = vxlan_vni_field(vx_vni);
 }
+#endif
 
 static int mlx5e_encap_entry_attach_update(struct mlx5e_priv *priv,
 					   struct net_device *out_dev,
@@ -3201,6 +3691,7 @@ out:
 	return err;
 }
 
+#ifdef __IPV6_SUPPORT__
 static int mlx5e_create_encap_header_ipv6(struct mlx5e_priv *priv,
 					  struct net_device *mirred_dev,
 					  struct mlx5e_encap_entry *e)
@@ -3322,10 +3813,15 @@ out:
 		neigh_release(n);
 	return err;
 }
+#endif
 
 bool mlx5e_encap_take(struct mlx5e_encap_entry *e)
 {
+#ifdef HAVE_REFCOUNT
 	return refcount_inc_not_zero(&e->refcnt);
+#else
+	return atomic_inc_not_zero(&e->refcnt);
+#endif
 }
 
 static struct mlx5e_encap_entry *
@@ -3378,12 +3874,21 @@ mlx5e_encap_get_create(struct mlx5e_priv
 	e->tunnel_type = tunnel_type;
 	INIT_LIST_HEAD(&e->flows);
 	INIT_LIST_HEAD(&e->neigh_update_list);
+#ifdef HAVE_REFCOUNT
 	refcount_set(&e->refcnt, 1);
+#else
+	atomic_set(&e->refcnt, 1);
+#endif
 
 	if (family == AF_INET)
 		err = mlx5e_create_encap_header_ipv4(priv, mirred_dev, e);
+#ifdef __IPV6_SUPPORT__
 	else if (family == AF_INET6)
 		err = mlx5e_create_encap_header_ipv6(priv, mirred_dev, e);
+#else
+	else
+		err = -EOPNOTSUPP;
+#endif
 
 	if (err && err != -EAGAIN) {
 		kfree(e);
@@ -3411,8 +3916,12 @@ static int mlx5e_attach_encap(struct mlx
 			      struct ip_tunnel_info *tun_info,
 			      struct net_device *mirred_dev,
 			      struct net_device **encap_dev,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			      struct mlx5e_tc_flow *flow,
 			      struct netlink_ext_ack *extack)
+#else
+			      struct mlx5e_tc_flow *flow)
+#endif
 {
 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
 	struct ip_tunnel_key *key = &tun_info->key;
@@ -3426,8 +3935,10 @@ static int mlx5e_attach_encap(struct mlx
 	/* setting udp src port isn't supported */
 	if (memchr_inv(&key->tp_src, 0, sizeof(key->tp_src))) {
 vxlan_encap_offload_err:
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "must set udp dst port and not set udp src port");
+#endif
 		netdev_warn(priv->netdev,
 			    "must set udp dst port and not set udp src port\n");
 		return -EOPNOTSUPP;
@@ -3437,8 +3948,10 @@ vxlan_encap_offload_err:
 	    MLX5_CAP_ESW(priv->mdev, vxlan_encap_decap)) {
 		tunnel_type = MLX5_REFORMAT_TYPE_L2_TO_VXLAN;
 	} else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "port isn't an offloaded vxlan udp dport");
+#endif
 		netdev_warn(priv->netdev,
 			    "%d isn't an offloaded vxlan udp dport\n", be16_to_cpu(key->tp_dst));
 		return -EOPNOTSUPP;
@@ -3464,11 +3977,16 @@ vxlan_encap_offload_err:
 	return err;
 }
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 static int add_vlan_rewrite_action(struct mlx5e_priv *priv,
 				   const struct tc_action *a,
 				   struct mlx5_esw_flow_attr *attr,
 				   struct mlx5e_tc_flow_parse_attr *parse_attr,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				   u32 *action, struct netlink_ext_ack *extack)
+#else
+				   u32 *action)
+#endif
 {
 	int err;
 	struct tcf_pedit_key_ex pedit_key_ex = {
@@ -3489,23 +4007,36 @@ static int add_vlan_rewrite_action(struc
 	};
 
 	if (tcf_vlan_push_prio(a)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "Setting VLAN prio is not supported");
+#else
+		pr_err("Setting VLAN prio is not supported");
+#endif
 		return -EOPNOTSUPP;
 	}
 
 	err = parse_tc_pedit_action(priv, (const struct tc_action *)&pedit_act,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				    MLX5_FLOW_NAMESPACE_FDB, parse_attr, NULL);
+#else
+				    MLX5_FLOW_NAMESPACE_FDB, parse_attr);
+#endif
 	*action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;
 	attr->mirror_count = attr->out_count;
 	return err;
 }
+#endif
 
 static int parse_tc_vlan_action(struct mlx5e_priv *priv,
 				const struct tc_action *a,
 				struct mlx5_esw_flow_attr *attr,
 				u32 *action,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				struct mlx5e_tc_flow_parse_attr *parse_attr,
 				struct netlink_ext_ack *extack)
+#else
+				struct mlx5e_tc_flow_parse_attr *parse_attr)
+#endif
 {
 	u8 vlan_idx = attr->total_vlan;
 
@@ -3523,12 +4054,18 @@ static int parse_tc_vlan_action(struct m
 			*action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_POP;
 		}
 	} else if (tcf_vlan_action(a) == TCA_VLAN_ACT_PUSH) {
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 		if ((*action) & MLX5_FLOW_CONTEXT_ACTION_VLAN_POP) {
 			/* Replace vlan pop+push with vlan modify */
 			*action &= ~MLX5_FLOW_CONTEXT_ACTION_VLAN_POP;
 			return add_vlan_rewrite_action(priv, a, attr, parse_attr,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 					       action, extack);
+#else
+					       action);
+#endif
 		}
+#endif
 		attr->vlan_vid[vlan_idx] = tcf_vlan_push_vid(a);
 		attr->vlan_prio[vlan_idx] = tcf_vlan_push_prio(a);
 		attr->vlan_proto[vlan_idx] = tcf_vlan_push_proto(a);
@@ -3550,48 +4087,104 @@ static int parse_tc_vlan_action(struct m
 			*action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_PUSH;
 		}
 	} else { /* action is TCA_VLAN_ACT_MODIFY */
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 		return add_vlan_rewrite_action(priv, a, attr, parse_attr,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 					       action, extack);
+#else
+					       action);
+#endif
+#else
+		return -EOPNOTSUPP;
+#endif
 	}
 
 	attr->total_vlan = vlan_idx + 1;
 
 	return 0;
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static int parse_tc_fdb_actions(struct mlx5e_priv *priv, struct tcf_exts *exts,
 				struct mlx5e_tc_flow_parse_attr *parse_attr,
-				struct mlx5e_tc_flow *flow,
-				struct netlink_ext_ack *extack)
+#ifdef HAVE_TCF_TUNNEL_INFO
+				struct mlx5e_tc_flow *flow
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+				,struct netlink_ext_ack *extack)
+#else
+				)
+#endif /* HAVE_TC_CLS_OFFLOAD_EXTACK */
+#else
+				struct mlx5_esw_flow_attr *attr)
+#endif
 {
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && defined (HAVE_IS_TCF_GACT_GOTO_CHAIN)
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#endif
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+#endif
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+#ifdef HAVE_TCF_TUNNEL_INFO
+#ifndef CONFIG_COMPAT_TCF_TUNNEL_KEY_MOD
 	struct ip_tunnel_info *info = NULL;
+#else
+	struct ip_tunnel_info info_compat;
+	struct ip_tunnel_info *info = &info_compat;
+#endif
+#endif
 	const struct tc_action *a;
+#ifdef tcf_exts_for_each_action
+	int i;
+#else
 	LIST_HEAD(actions);
+#endif
+#ifdef HAVE_TCF_TUNNEL_INFO
 	bool encap = false;
+#endif
 	u32 action = 0;
-	int err, i;
+	int err;
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	attr->parse_attr = parse_attr;
+#endif
 
+#ifdef HAVE_TCF_EXTS_HAS_ACTIONS
 	if (!tcf_exts_has_actions(exts))
+#else
+	if (tc_no_actions(exts))
+#endif
 		return -EINVAL;
 
 	attr->in_rep = rpriv->rep;
 	attr->in_mdev = priv->mdev;
 
+#ifdef HAVE_TCF_EXTS_TO_LIST
+	tcf_exts_to_list(exts, &actions);
+	list_for_each_entry(a, &actions, list) {
+#else
+#ifdef tcf_exts_for_each_action
 	tcf_exts_for_each_action(i, a, exts) {
+#else
+	tc_for_each_action(a, exts) {
+#endif
+#endif
+#ifdef HAVE_IS_TCF_GACT_SHOT
 		if (is_tcf_gact_shot(a)) {
 			action |= MLX5_FLOW_CONTEXT_ACTION_DROP |
 				  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			continue;
 		}
+#endif
 
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 		if (is_tcf_pedit(a)) {
 			err = parse_tc_pedit_action(priv, a, MLX5_FLOW_NAMESPACE_FDB,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 						    parse_attr, extack);
+#else
+						    parse_attr);
+#endif
 			if (err)
 				return err;
 
@@ -3599,25 +4192,39 @@ static int parse_tc_fdb_actions(struct m
 			attr->mirror_count = attr->out_count;
 			continue;
 		}
+#endif
 
+#ifdef HAVE_TCA_CSUM_UPDATE_FLAG_IPV4HDR
 		if (is_tcf_csum(a)) {
 			if (csum_offload_supported(priv, action,
-						   tcf_csum_update_flags(a),
-						   extack))
+						   tcf_csum_update_flags(a)
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+						   ,extack))
+#else
+						   ))
+#endif
 				continue;
 
 			return -EOPNOTSUPP;
 		}
+#endif
 
 		if (is_tcf_mirred_egress_redirect(a) || is_tcf_mirred_egress_mirror(a)) {
 			struct mlx5e_priv *out_priv;
 			struct net_device *out_dev;
 
+#ifdef HAVE_TCF_MIRRED_DEV
 			out_dev = tcf_mirred_dev(a);
+#else
+			int ifindex = tcf_mirred_ifindex(a);
+			out_dev = __dev_get_by_index(dev_net(priv->netdev), ifindex);
+#endif
 
 			if (attr->out_count >= MLX5_MAX_FLOW_FWD_VPORTS) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "can't support more output ports, can't offload forwarding");
+#endif
 				pr_err("can't support more than %d output ports, can't offload forwarding\n",
 				       attr->out_count);
 				return -EOPNOTSUPP;
@@ -3643,25 +4250,35 @@ static int parse_tc_fdb_actions(struct m
 				rpriv = out_priv->ppriv;
 				attr->out_rep[attr->out_count] = rpriv->rep;
 				attr->out_mdev[attr->out_count++] = out_priv->mdev;
+#ifdef HAVE_TCF_TUNNEL_INFO
 			} else if (encap) {
+#ifdef HAVE_TCF_MIRRED_DEV
 				parse_attr->mirred_ifindex = out_dev->ifindex;
+#else
+				parse_attr->mirred_ifindex = ifindex;
+#endif
 				parse_attr->tun_info = *info;
 				action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT |
 					  MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 					  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 				/* attr->out_rep is resolved when we handle encap */
 			} else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "devices are not on same switch HW, can't offload forwarding");
+#endif
 				pr_err("devices %s %s not on same switch HW, can't offload forwarding\n",
 				       priv->netdev->name, out_dev->name);
 				return -EINVAL;
 			}
 			continue;
 		}
-
 		if (is_tcf_tunnel_set(a)) {
+#if !defined(CONFIG_COMPAT_TCF_TUNNEL_KEY_MOD) || defined (CONFIG_NET_SCHED_NEW)
 			info = tcf_tunnel_info(a);
+#else
+			tcf_tunnel_info_compat(a, info);
+#endif
 			if (info)
 				encap = true;
 			else
@@ -3669,10 +4286,29 @@ static int parse_tc_fdb_actions(struct m
 			attr->mirror_count = attr->out_count;
 			continue;
 		}
+#else /* HAVE_TCF_TUNNEL_INFO */
+			} else {
+				pr_err("devices %s %s not on same switch HW, can't offload forwarding\n",
+				       priv->netdev->name, out_dev->name);
+				return -EINVAL;
+			}
+			action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
+				  MLX5_FLOW_CONTEXT_ACTION_COUNT;
+			out_priv = netdev_priv(out_dev);
+			attr->out_rep[attr->out_count++] = out_priv->ppriv;
+			continue;
+		}
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
+#ifdef HAVE_TCF_TUNNEL_INFO
+#ifdef HAVE_IS_TCF_VLAN
 		if (is_tcf_vlan(a)) {
 			err = parse_tc_vlan_action(priv, a, attr, &action,
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 						   parse_attr, extack);
+#else
+						   parse_attr);
+#endif
 
 			if (err)
 				return err;
@@ -3680,22 +4316,33 @@ static int parse_tc_fdb_actions(struct m
 			attr->mirror_count = attr->out_count;
 			continue;
 		}
+#endif
 
 		if (is_tcf_tunnel_release(a)) {
 			action |= MLX5_FLOW_CONTEXT_ACTION_DECAP;
 			continue;
 		}
+#endif
 
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && defined (HAVE_IS_TCF_GACT_GOTO_CHAIN)
 		if (is_tcf_gact_goto_chain(a)) {
 			u32 dest_chain = tcf_gact_goto_chain_index(a);
 			u32 max_chain = mlx5_eswitch_get_chain_range(esw);
 
 			if (dest_chain <= attr->chain) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG(extack, "Goto earlier chain isn't supported");
+#else
+				netdev_warn(priv->netdev, "Goto earlier chain isn't supported\n");
+#endif
 				return -EOPNOTSUPP;
 			}
 			if (dest_chain > max_chain) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG(extack, "Requested destination chain is out of supported range");
+#else
+				netdev_warn(priv->netdev, "Requested destination chain is out of supported range\n");
+#endif
 				return -EOPNOTSUPP;
 			}
 			action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
@@ -3703,25 +4350,38 @@ static int parse_tc_fdb_actions(struct m
 
 			continue;
 		}
+#endif /*HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON  HAVE_IS_TCF_GACT_GOTO_CHAIN */
 
 		return -EINVAL;
 	}
 
 	attr->action = action;
+#ifdef HAVE_TCF_TUNNEL_INFO
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	if (!actions_match_supported(priv, exts, parse_attr, flow, extack))
+#else
+	if (!actions_match_supported(priv, exts, parse_attr, flow))
+#endif
 		return -EOPNOTSUPP;
+#endif
 
 	if (attr->dest_chain) {
 		if (attr->action & MLX5_FLOW_CONTEXT_ACTION_FWD_DEST) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG(extack, "Mirroring goto chain rules isn't supported");
+#else
+			netdev_warn(priv->netdev, "Mirroring goto chain rules isn't supported\n");
+#endif
 			return -EOPNOTSUPP;
 		}
 		attr->action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
 	}
 
 	if (attr->mirror_count > 0 && !mlx5_esw_has_fwd_fdb(priv->mdev)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "current firmware doesn't support split rule for port mirroring");
+#endif
 		netdev_warn_once(priv->netdev, "current firmware doesn't support split rule for port mirroring\n");
 		return -EOPNOTSUPP;
 	}
@@ -3732,32 +4392,45 @@ static int parse_tc_fdb_actions(struct m
 static bool is_peer_flow_needed(struct mlx5e_tc_flow *flow)
 {
 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	bool is_rep_ingress = attr->in_rep->vport != MLX5_VPORT_UPLINK &&
 				atomic_read(&flow->flags) & MLX5E_TC_FLOW_INGRESS;
 	bool act_is_encap = !!(attr->action &
 			       MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT);
+#endif
 	bool esw_paired = mlx5_devcom_is_paired(attr->in_mdev->priv.devcom,
 						MLX5_DEVCOM_ESW_OFFLOADS);
 
 	if (!esw_paired)
 		return false;
 
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	if ((mlx5_lag_is_sriov(attr->in_mdev) ||
 	     mlx5_lag_is_multipath(attr->in_mdev)) &&
-	    (is_rep_ingress || act_is_encap))
+	    (is_rep_ingress || act_is_encap
+#ifdef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+	     || (atomic_read(&flow->flags) & MLX5E_TC_FLOW_EGRESS)
+#endif
+))
 		return true;
 
 	return false;
+#else
+	return (mlx5_lag_is_sriov(attr->in_mdev) ||  mlx5_lag_is_multipath(attr->in_mdev));
+#endif
 }
 
 static void get_flags(int flags, int *flow_flags)
 {
 	int __flow_flags = 0;
 
+#if !(defined(HAVE_NDO_SETUP_TC_4_PARAMS) || defined(HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX))
+	/* relevant for the new ndo */
 	if (flags & MLX5E_TC_INGRESS)
 		__flow_flags |= MLX5E_TC_FLOW_INGRESS;
 	if (flags & MLX5E_TC_EGRESS)
 		__flow_flags |= MLX5E_TC_FLOW_EGRESS;
+#endif
 
 	if (flags & MLX5E_TC_ESW_OFFLOAD)
 		__flow_flags |= MLX5E_TC_FLOW_ESWITCH;
@@ -3774,12 +4447,28 @@ static const struct rhashtable_params tc
 	.automatic_shrinking = true,
 };
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+static void get_new_flags(struct mlx5e_priv *priv, int *flags)
+{
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+
+	if (mlx5e_eswitch_rep(priv->netdev) &&
+	    MLX5_VPORT_MANAGER(priv->mdev) && esw->mode == SRIOV_OFFLOADS)
+		*flags |= MLX5E_TC_ESW_OFFLOAD;
+}
+#endif
+
 static struct rhashtable *get_tc_ht(struct mlx5e_priv *priv, int flags)
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 	struct mlx5e_rep_priv *uplink_rpriv;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	if (mlx5e_eswitch_rep(priv->netdev) &&
+	    MLX5_VPORT_MANAGER(priv->mdev) && esw->mode == SRIOV_OFFLOADS) {
+#else
 	if (flags & MLX5E_TC_ESW_OFFLOAD) {
+#endif
 		uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
 		return &uplink_rpriv->tc_ht;
 	} else /* NIC offload */
@@ -3832,11 +4521,19 @@ mlx5e_alloc_flow(struct mlx5e_priv *priv
 	flow->cookie = f->cookie;
 	atomic_set(&flow->flags, flow_flags);
 	flow->priv = priv;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	INIT_LIST_HEAD(&flow->encap);
+#endif
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	INIT_LIST_HEAD(&flow->mod_hdr);
+#endif
 	INIT_LIST_HEAD(&flow->hairpin);
 	INIT_LIST_HEAD(&flow->tmp_list);
+#ifdef HAVE_REFCOUNT
 	refcount_set(&flow->refcnt, 1);
+#else
+	atomic_set(&flow->refcnt, 1);
+#endif
 
 	err = parse_cls_flower(priv, flow, &parse_attr->spec, f);
 	if (err)
@@ -3862,7 +4559,9 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 		     struct mlx5e_tc_flow **__flow)
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
 	int attr_size, err;
@@ -3873,10 +4572,21 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 			       &parse_attr, &flow);
 	if (err)
 		goto out;
-
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && defined (HAVE_IS_TCF_GACT_GOTO_CHAIN)
 	flow->esw_attr->chain = f->common.chain_index;
 	flow->esw_attr->prio = TC_H_MAJ(f->common.prio) >> 16;
+#else
+	flow->esw_attr->prio = 1;
+#endif
+#ifdef HAVE_TCF_TUNNEL_INFO
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow, extack);
+#else
+	err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow);
+#endif /*HAVE_TC_CLS_OFFLOAD_EXTACK */
+#else
+	err = parse_tc_fdb_actions(priv, f->exts, parse_attr, flow->esw_attr);
+#endif
 	if (err)
 		goto err_free;
 
@@ -3889,7 +4599,11 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 	else
 		flow->esw_attr->counter_dev = priv->mdev;
 
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	err = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow, extack);
+#else
+	err = mlx5e_tc_add_fdb_flow(priv, parse_attr, flow);
+#endif
 	if (err) {
 		if (!(err == -ENETUNREACH && mlx5_lag_is_multipath(in_mdev)))
 			goto err_free;
@@ -3900,12 +4614,16 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 	if (!(flow->esw_attr->action &
 	      MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT)) {
 		kvfree(parse_attr);
+#ifdef HAVE_TCF_TUNNEL_INFO
 		flow->esw_attr->parse_attr = NULL;
+#endif
 	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	err = mlx5e_tc_update_and_init_done_fdb_flow(priv, flow);
 	if (err)
 		goto err_free;
+#endif
 
 	*__flow = flow;
 
@@ -4001,14 +4719,20 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 		   int flow_flags,
 		   struct mlx5e_tc_flow **__flow)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
 	int attr_size, err;
 
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && defined (HAVE_IS_TCF_GACT_GOTO_CHAIN)
 	/* multi-chain not supported for NIC rules */
 	if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
 		return -EOPNOTSUPP;
+#endif
+#endif
 
 	flow_flags |= MLX5E_TC_FLOW_NIC;
 	attr_size  = sizeof(struct mlx5_nic_flow_attr);
@@ -4017,17 +4741,27 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 	if (err)
 		goto out;
 
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow, extack);
+#else
+	err = parse_tc_nic_actions(priv, f->exts, parse_attr, flow);
+#endif
 	if (err)
 		goto err_free;
 
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	err = mlx5e_tc_add_nic_flow(priv, parse_attr, flow, extack);
+#else
+	err = mlx5e_tc_add_nic_flow(priv, parse_attr, flow);
+#endif
 	if (err)
 		goto err_free;
 
 	mlx5e_set_flow_flag_mb_before(flow, MLX5E_TC_FLOW_OFFLOADED |
 				      MLX5E_TC_FLOW_INIT_DONE);
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	kfree(parse_attr->mod_hdr_actions);
+#endif
 	kvfree(parse_attr);
 	*__flow = flow;
 
@@ -4035,7 +4769,9 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 
 err_free:
 	mlx5e_flow_put(priv, flow);
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	kfree(parse_attr->mod_hdr_actions);
+#endif
 	kvfree(parse_attr);
 out:
 	return err;
@@ -4053,9 +4789,10 @@ mlx5e_tc_add_flow(struct mlx5e_priv *pri
 
 	get_flags(flags, &flow_flags);
 
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON)
 	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
 		return -EOPNOTSUPP;
-
+#endif
 	if (esw && esw->mode == SRIOV_OFFLOADS)
 		err = mlx5e_add_fdb_flow(priv, f, flow_flags, flow);
 	else
@@ -4067,17 +4804,25 @@ mlx5e_tc_add_flow(struct mlx5e_priv *pri
 int mlx5e_configure_flower(struct mlx5e_priv *priv,
 			   struct tc_cls_flower_offload *f, int flags)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 	struct mlx5e_tc_flow *flow;
 	int err = 0;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	get_new_flags(priv, &flags);
+#endif
+
 	rcu_read_lock();
 	flow = rhashtable_lookup(tc_ht, &f->cookie, tc_ht_params);
 	if (flow) {
 		rcu_read_unlock();
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "flow cookie already exists, ignoring");
+#endif
 		netdev_warn_once(priv->netdev,
 				 "flow cookie %lx already exists, ignoring\n",
 				 f->cookie);
@@ -4100,17 +4845,24 @@ err_free:
 out:
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_configure_flower);
+#endif
 
 #define DIRECTION_MASK (MLX5E_TC_INGRESS | MLX5E_TC_EGRESS)
 #define FLOW_DIRECTION_MASK (MLX5E_TC_FLOW_INGRESS | MLX5E_TC_FLOW_EGRESS)
 
 static bool same_flow_direction(struct mlx5e_tc_flow *flow, int flags)
 {
+#if !(defined(HAVE_NDO_SETUP_TC_4_PARAMS) || defined(HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX))
 	if ((atomic_read(&flow->flags) & FLOW_DIRECTION_MASK) ==
 	    (flags & DIRECTION_MASK))
 		return true;
 
 	return false;
+#else
+	return true;
+#endif
 }
 
 int mlx5e_delete_flower(struct mlx5e_priv *priv,
@@ -4134,7 +4886,11 @@ int mlx5e_delete_flower(struct mlx5e_pri
 
 	return 0;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_delete_flower);
+#endif
 
+#ifdef HAVE_TC_CLSFLOWER_STATS
 int mlx5e_stats_flower(struct mlx5e_priv *priv,
 		       struct tc_cls_flower_offload *f, int flags)
 {
@@ -4143,11 +4899,19 @@ int mlx5e_stats_flower(struct mlx5e_priv
 	struct mlx5_eswitch *peer_esw;
 	struct mlx5e_tc_flow *flow;
 	struct mlx5_fc *counter;
+#ifndef HAVE_TCF_EXTS_STATS_UPDATE
+	struct tc_action *a;
+	LIST_HEAD(actions);
+#endif
 	int err = 0;
 	u64 lastuse = 0;
 	u64 packets = 0;
 	u64 bytes = 0;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	get_new_flags(priv, &flags);
+#endif
+
 	rcu_read_lock();
 	flow = mlx5e_flow_get(rhashtable_lookup(tc_ht, &f->cookie,
 						tc_ht_params));
@@ -4193,11 +4957,40 @@ int mlx5e_stats_flower(struct mlx5e_priv
 no_peer_counter:
 	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
 out:
+#ifdef HAVE_TCF_EXTS_STATS_UPDATE
 	tcf_exts_stats_update(f->exts, bytes, packets, lastuse);
+#else
+	preempt_disable();
+
+#ifdef HAVE_TCF_EXTS_TO_LIST
+	tcf_exts_to_list(f->exts, &actions);
+	list_for_each_entry(a, &actions, list)
+#else
+	tc_for_each_action(a, f->exts)
+#endif
+#ifdef HAVE_TCF_ACTION_STATS_UPDATE
+	tcf_action_stats_update(a, bytes, packets, lastuse);
+#else
+	{
+		struct tcf_act_hdr *h = a->priv;
+
+		spin_lock(&h->tcf_lock);
+		h->tcf_tm.lastuse = max_t(u64, h->tcf_tm.lastuse, lastuse);
+		h->tcf_bstats.bytes += bytes;
+		h->tcf_bstats.packets += packets;
+		spin_unlock(&h->tcf_lock);
+	}
+#endif
+	preempt_enable();
+#endif
 errout:
 	mlx5e_flow_put(priv, flow);
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_stats_flower);
+#endif
+#endif
 
 static void mlx5e_tc_hairpin_update_dead_peer(struct mlx5e_priv *priv,
 					      struct mlx5e_priv *peer_priv)
@@ -4244,15 +5037,19 @@ static int mlx5e_tc_netdev_event(struct
 
 	return NOTIFY_DONE;
 }
+#endif /* HAVE_TC_FLOWER_OFFLOAD */
 
 int mlx5e_tc_nic_init(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_table *tc = &priv->fs.tc;
 	int err;
 
 	mutex_init(&tc->t_lock);
 	spin_lock_init(&tc->mod_hdr_tbl_lock);
+#ifdef HAVE_TCF_PEDIT_TCFP_KEYS_EX
 	hash_init(tc->mod_hdr_tbl);
+#endif
 	spin_lock_init(&tc->hairpin_tbl_lock);
 	hash_init(tc->hairpin_tbl);
 
@@ -4268,8 +5065,12 @@ int mlx5e_tc_nic_init(struct mlx5e_priv
 	}
 
 	return err;
+#else
+	return 0;
+#endif
 }
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 static void _mlx5e_tc_del_flow(void *ptr, void *arg)
 {
 	struct mlx5e_tc_flow *flow = ptr;
@@ -4278,9 +5079,11 @@ static void _mlx5e_tc_del_flow(void *ptr
 	mlx5e_tc_del_flow(priv, flow);
 	kfree(flow);
 }
+#endif
 
 void mlx5e_tc_nic_cleanup(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_table *tc = &priv->fs.tc;
 
 	if (tc->netdevice_nb.notifier_call)
@@ -4293,35 +5096,48 @@ void mlx5e_tc_nic_cleanup(struct mlx5e_p
 		tc->t = NULL;
 	}
 	mutex_destroy(&tc->t_lock);
+#endif
 }
 
 int mlx5e_tc_esw_init(struct rhashtable *tc_ht)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	return rhashtable_init(tc_ht, &tc_ht_params);
+#else
+	return 0;
+#endif
 }
 
 void mlx5e_tc_esw_cleanup(struct rhashtable *tc_ht)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	rhashtable_free_and_destroy(tc_ht, _mlx5e_tc_del_flow, NULL);
+#endif
 }
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 int mlx5e_tc_num_filters(struct mlx5e_priv *priv, int flags)
 {
 	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 
 	return atomic_read(&tc_ht->nelems);
 }
+#endif
 
 void mlx5e_tc_clean_fdb_peer_flows(struct mlx5_eswitch *esw)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_flow *flow, *tmp;
 
 	list_for_each_entry_safe(flow, tmp, &esw->offloads.peer_flows, peer)
 		__mlx5e_tc_del_fdb_peer_flow(flow);
+#endif
 }
 
 void mlx5e_tc_reoffload_flows_work(struct mlx5_core_dev *mdev)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct mlx5_eswitch *esw = mdev->priv.eswitch;
 	struct mlx5e_rep_priv *rpriv;
 	struct mlx5e_tc_flow *flow, *tmp;
@@ -4332,7 +5148,13 @@ void mlx5e_tc_reoffload_flows_work(struc
 	list_for_each_entry_safe(flow, tmp, &rpriv->unready_flows, unready) {
 		if (!mlx5e_tc_add_fdb_flow(flow->priv,
 					   flow->esw_attr->parse_attr,
-					   flow, NULL))
+					   flow
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
+					   , NULL
+#endif
+					   ))
 			remove_unready_flow(flow);
 	}
+#endif
+#endif
 }
